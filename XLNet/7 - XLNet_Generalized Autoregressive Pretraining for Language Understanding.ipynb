{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a5a169",
   "metadata": {},
   "source": [
    "- 작성자 : 220200013 이해중(예시 추가)\n",
    "\n",
    "### XLNet: Generalized Autoregressive Pretraining for Language Understanding (Motivation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b9252",
   "metadata": {},
   "source": [
    "\n",
    "- GPT-1과 BERT는 Transfer Learning, Self-supervised Learning, Transformer를 사용했다는 공통점 존재\n",
    "- 자연어 처리 분야에 한 획을 그은 GPT-1과 BERT가 이전에 나옴\n",
    "- BERT의 **Masked Language Model**의 단점을 사람이 실제로 언어를 배우는 방식과의 차이를 생각해보며 떠올려봄\n",
    "- XLNet 논문을 정리\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a708d",
   "metadata": {},
   "source": [
    "#### Recent Trends\n",
    "\n",
    "- Transformer model에서 사용된 self-attention block은 General한 Encoder/Decoder로써 다양한 NLP task에 좋은 성능을 내서 활발히 사용되고 있음  \n",
    "- Transformer는 self-attention block을 대략 6개 쌓아서 사용했지만, 이후의 논문(e.g. BERT, GPT-3, XLNet, ALBERT, RoBERTa, Reformer, T5, ELECTRA 등)은 self-supervised learning framework를 통해 Tansformer를 학습(block을 12개, 24개, 그 이상의 layer를 깊게 쌓은 모델을 대규모 학습데이터를 통해 학습)시킨 뒤 한 뒤 Transfer Learning을 통해 다양한 NLP task를 발전시킴 \n",
    "- NLP 분야 이외에 다른 분야도 self-attention 및 transformer 구조를 사용하고 있음. 예를 들어 recommender systems, drug discovery, computer vision, ...\n",
    "\n",
    "- 하지만, 여전히 self-attention models은 natural language generation 에서 사용되는 greedy decoding framework(e.g. 단어를 생성하는 과정인 <SOS> token부터 시작해서 <EOD> token이 나올때까지 단어를 생성)을 벗어나지 못하는 단점 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bcc779",
   "metadata": {},
   "source": [
    "### BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "- 현재까지 가장 널리 쓰이는 Pre-train model, 기본적으로 BERT도 GPT와 마찬가지로 Language Modeling이라는 Task로 문장의 일부(다음) 단어를 예측함으로써 pre-training을 수행한 모델\n",
    "- 참고로 Transformer model 이전에 Bi-LSTM 기반의 encoder로 Pre-train 했던 ELMo가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4b675",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- 언어 모델은 left-context 또는 right-context만 사용하여 다음 단어를 예측하는 한계점 존재\n",
    "- 하지만 실제 사람의 이해하는 언어는 bi-directional 방식 (e.g. 통화 도중 통신문제로 중간이 내용이 잘린 경우 뒤쪽 문맥을 이용하여 못들은 내용 유추 가능) \n",
    "- 이러한 Motivation을 활용해 나온것이 `Masked Language Model (MLM)`이며, 이를 Pre-training Task로 진행\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886073f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T04:24:56.200399Z",
     "start_time": "2022-11-25T04:24:56.124524Z"
    }
   },
   "source": [
    "#### Pre-training Tasks in BERT\n",
    "\n",
    "- **Masked Language Model (MLM)**\n",
    "    - Problem : \n",
    "        - Mask token never seen during fine-tuning\n",
    "    \n",
    "    - Solution\n",
    "        - input token의 일부 비율을 임의로 masking한 다음 마스킹된 token을 예측함\n",
    "        - 예측할 단어의 15%(hyperparameter)를 MASK로 치환\n",
    "    \n",
    "            - 모두 [MASK]로 치환하면 부작용 발생하기 때문에 아래의 비율로 또 구분함\n",
    "                - 80% of the time, replace with [MASK]\n",
    "                    - went to the store → went to the [MASK]\n",
    "                - 10% of the time, replace with a random word\n",
    "                    - went to the store → went to the running\n",
    "                - 10% of the time, keep the sentence as same\n",
    "                    - went to the store → went to the store\n",
    "                    \n",
    "        - Masking이 너무 많은 경우 : Too expensive to train. (계산, 시간 많이 걸림)\n",
    "        - Masking이 너무 적은 경우 : Not enough to capture context. \n",
    "    \n",
    "BERT에서 쓰인 pre-training 기법은 위에서 설명한  MLM task 이외에 GPT에서도 존재하던 문장 레벨에서의 task를 대응하기 위한 pre-traing 기법을 추가 제안 (Next Sentence Prediction)\n",
    "\n",
    "\n",
    "\n",
    "- **Next Sentence Prediction**\n",
    "    - To learn the relationships among sentences, predict whether Sentence B is an actual  sentence that proceeds Sentence A, or a random sentence\n",
    "    - 즉, 주어진 두 문장이 서로 연관이 있는지 없는지 수행하는 binary classification task 함   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6df7f41a",
   "metadata": {},
   "source": [
    "###  Auto Encoding (AE) vs Autoregression (AR) 비교\n",
    "\n",
    "- **Pre-training의 대표적인 objective는 아래 두 방법으로 각각 AE 및 AR 존재**\n",
    "    - AE의 목적 : masked된 token을 예측하는 것\n",
    "    - AR의 목적 : 일반적인 language modeling (주어진 sequence로 next word를 예측)\n",
    "    \n",
    "    \n",
    "![image.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet1.png?raw=true)\n",
    "\n",
    "- **XLNet은 AE의 문제점 및 AR의 문제점을 극복하고자 함**\n",
    "    - AE의 문제점 : \n",
    "        - [MASK] token이 독립적으로 예측 $\\color{red}{\\text{(independent assumption)}}$되기 때문에 token 사이의 dependency는 학습할 수 없음\n",
    "        - Finetuning 과정에서 [MASK] token이 등장하지 않기때문에 pretraining과 finetuning사이에 $\\color{red}{decrepancy}$ 발생\n",
    "        \n",
    "    - AR의 문제점 : \n",
    "        - $\\color{red}{\\text{단일 방향 정보}}$만 이용하여 학습 가능함\n",
    "        \n",
    " \n",
    "![image-2.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet2.jpg.png?raw=true)\n",
    " \n",
    "- XLNet의 Points를 차례대로 살펴보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842d57e",
   "metadata": {},
   "source": [
    "### XLNet details (3 types 설명 포함)\n",
    "XLNet의 세 가지 특징을 하나하나씩 살펴보는 그림 예시 및 설명\n",
    "\n",
    "\n",
    "- **4.1 Permutation Language Modeling Objective**\n",
    "- **4.2 Two-Stream Self-Attention**\n",
    "- **4.3 Two-Stream Self-Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38505d5",
   "metadata": {},
   "source": [
    "#### Permutation Language Modeling Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e231b",
   "metadata": {},
   "source": [
    "![image-2.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23201efa",
   "metadata": {},
   "source": [
    "![image-2.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60e86e",
   "metadata": {},
   "source": [
    "#### Target-Aware Representation for Transformer\n",
    "- 새로운 object function은 standard Transformation에서 작동하지 않음\n",
    "- 따라서 Transformer에 XLNet의 object function을 적용하기 위해 $\\color{blue}{\\text{Target-Aware Representation}}$을 제안"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bd076cf",
   "metadata": {},
   "source": [
    "![image-2.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet6.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5968cd",
   "metadata": {},
   "source": [
    "#### Two-Stream Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb9d4a",
   "metadata": {},
   "source": [
    "![image-2.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet7.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc897a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:29:46.747949Z",
     "start_time": "2022-12-18T18:29:46.737948Z"
    }
   },
   "source": [
    "![image-2.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/xlnet8.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4ff96",
   "metadata": {},
   "source": [
    "### XLnet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a91cd",
   "metadata": {},
   "source": [
    "- 논문에서 설정한 Hyperparameter 는 다음과 같다. 본 과제는 Resource 제약으로 다르게 설정함\n",
    "\n",
    "![](https://github.com/graykode/xlnet-Pytorch/blob/master/images/hyperparameters.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe3f50",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b77e3",
   "metadata": {},
   "source": [
    "#### XLNet model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a8a9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:04:57.336668Z",
     "start_time": "2022-12-18T18:04:54.914296Z"
    }
   },
   "outputs": [],
   "source": [
    "# library 불러오기\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d49e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:04:57.383169Z",
     "start_time": "2022-12-18T18:04:57.338668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:354: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:354: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\lhj91\\AppData\\Local\\Temp\\ipykernel_36324\\515873358.py:354: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if bi_data and bsz%2 is 0:\n"
     ]
    }
   ],
   "source": [
    "class XLNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Defines a Transformer-XL computation graph with additional\n",
    "        support for XLNet.\n",
    "        Args:\n",
    "        inp_k: int32 Tensor in shape [len, bsz], the input token IDs.\n",
    "        seg_id: int32 Tensor in shape [len, bsz], the input segment IDs.\n",
    "        input_mask: float32 Tensor in shape [len, bsz], the input mask.\n",
    "          0 for real tokens and 1 for padding.\n",
    "        mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory\n",
    "          from previous batches. The length of the list equals n_layer.\n",
    "          If None, no memory is used.\n",
    "        perm_mask: float32 Tensor in shape [len, len, bsz].\n",
    "          If perm_mask[i, j, k] = 0, i attend to j in batch k;\n",
    "          if perm_mask[i, j, k] = 1, i does not attend to j in batch k.\n",
    "          If None, each position attends to all the others.\n",
    "        target_mapping: float32 Tensor in shape [num_predict, len, bsz].\n",
    "          If target_mapping[i, j, k] = 1, the i-th predict in batch k is\n",
    "          on the j-th token.\n",
    "          Only used during pretraining for partial prediction.\n",
    "          Set to None during finetuning.\n",
    "        inp_q: float32 Tensor in shape [len, bsz].\n",
    "          1 for tokens with losses and 0 for tokens without losses.\n",
    "          Only used during pretraining for two-stream attention.\n",
    "          Set to None during finetuning.\n",
    "        n_layer: int, the number of layers.\n",
    "        d_model: int, the hidden size.\n",
    "        n_head: int, the number of attention heads.\n",
    "        d_head: int, the dimension size of each attention head.\n",
    "        d_inner: int, the hidden size in feed-forward layers.\n",
    "        ff_activation: str, \"relu\" or \"gelu\".\n",
    "        n_token: int, the vocab size.\n",
    "        dropout: float, dropout rate.\n",
    "        dropatt: float, dropout rate on attention probabilities.\n",
    "        mem_len: int, the number of tokens to cache.\n",
    "        reuse_len: int, the number of tokens in the currect batch to be cached\n",
    "          and reused in the future.\n",
    "        bi_data: bool, whether to use bidirectional input pipeline.\n",
    "          Usually set to True during pretraining and False during finetuning.\n",
    "        clamp_len: int, clamp all relative distances larger than clamp_len.\n",
    "          -1 means no clamping.\n",
    "      \"\"\"\n",
    "    def __init__(self, n_token, n_layer, n_head, d_head, d_inner, d_model, dropout, dropatt,\n",
    "                 attn_type, bi_data, clamp_len, same_length, reuse_len, mem_len):\n",
    "        super(XLNet, self).__init__()\n",
    "\n",
    "        self.n_token = n_token\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "        self.d_inner = d_inner\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.dropatt = dropatt\n",
    "        self.attn_type = attn_type\n",
    "        self.bi_data = bi_data\n",
    "        self.clamp_len = clamp_len\n",
    "        self.same_length = same_length\n",
    "        self.reuse_len = reuse_len\n",
    "        self.mem_len = mem_len\n",
    "\n",
    "        self.embedding = nn.Embedding(n_token, d_model)\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "        self.DropAttn = nn.Dropout(p=dropatt)\n",
    "\n",
    "        self.r_w_bias = nn.Parameter(torch.randn(self.n_layer,\n",
    "                                                  self.n_head,self.d_head))\n",
    "        self.r_r_bias = nn.Parameter(torch.randn(self.n_layer,\n",
    "                                                  self.n_head, self.d_head))\n",
    "\n",
    "        ##### Segment embedding\n",
    "        self.r_s_bias = nn.Parameter(torch.randn(self.n_layer,\n",
    "                                                 self.n_head,self.d_head))\n",
    "\n",
    "        self.seg_embed = nn.Parameter(torch.randn(self.n_layer, 2,\n",
    "                                                  self.n_head, self.d_head))\n",
    "\n",
    "        self.mask_emb = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # post-attention projection (back to `d_model`)\n",
    "        self.proj_o = nn.Parameter(torch.randn(self.d_model,\n",
    "                                                self.n_head, self.d_head))\n",
    "\n",
    "        #### Project hidden states to a specific head with a 4D-shape.\n",
    "        self.q_proj_weight = nn.Parameter(torch.randn(self.d_model,\n",
    "                                                       self.n_head, self.d_head))\n",
    "        self.k_proj_weight = nn.Parameter(torch.randn(self.d_model,\n",
    "                                                       self.n_head, self.d_head))\n",
    "        self.v_proj_weight = nn.Parameter(torch.randn(self.d_model,\n",
    "                                                       self.n_head, self.d_head))\n",
    "        self.r_proj_weight = nn.Parameter(torch.randn(self.d_model,\n",
    "                                                       self.n_head, self.d_head))\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.conv1 = nn.Linear(d_model, d_inner)\n",
    "        self.conv2 = nn.Linear(d_inner, d_model)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.softmax_b = nn.Parameter(torch.zeros(self.n_token))\n",
    "\n",
    "\n",
    "    def gelu(self, x):\n",
    "        \"\"\"Gaussian Error Linear Unit.\n",
    "        This is a smoother version of the RELU.\n",
    "        Original paper: https://arxiv.org/abs/1606.08415\n",
    "        Args:\n",
    "          x: float Tensor to perform activation.\n",
    "        Returns:\n",
    "          `x` with the GELU activation applied.\n",
    "        \"\"\"\n",
    "        cdf = 0.5 * (1.0 + torch.tanh(\n",
    "            (np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3)))))\n",
    "        return x * cdf\n",
    "\n",
    "    def rel_shift(self, x, klen=-1):\n",
    "        \"\"\"perform relative shift to form the relative attention score.\"\"\"\n",
    "        x_size = x.shape\n",
    "\n",
    "        x = torch.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "        x = x[1:, 0:, 0:, 0:] # tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "        x = torch.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "        x = x[0:, 0:klen, 0:, 0:] # tf.slice(x, [0, 0, 0, 0], [-1, klen, -1, -1])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def positionwise_ffn(self, inp, activation_type='relu'):\n",
    "\n",
    "        \"\"\"Position-wise Feed-forward Network.\"\"\"\n",
    "        output = self.conv1(inp)\n",
    "        output = self.Dropout(output)\n",
    "        if activation_type == 'relu':\n",
    "            output = self.relu(output)\n",
    "        elif activation_type == 'gelu':\n",
    "            output = self.gelu(output)\n",
    "        else:\n",
    "            raise ValueError('Unsupported activation type {}'.format(activation_type))\n",
    "\n",
    "        output = self.layer_norm(output + inp)\n",
    "        return output\n",
    "\n",
    "    def post_attention(self, h, attn_vec, residual=True):\n",
    "        \"\"\"Post-attention processing.\"\"\"\n",
    "\n",
    "        # post-attention projection (back to `d_model`)\n",
    "        attn_out = torch.einsum('ibnd,hnd->ibh', attn_vec, self.proj_o)\n",
    "\n",
    "        attn_out = self.Dropout(attn_out)\n",
    "        if residual:\n",
    "            output = self.layer_norm(attn_out + h)\n",
    "        else:\n",
    "            output = self.layer_norm(attn_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def head_projection(self, h, name):\n",
    "        \"\"\"Project hidden states to a specific head with a 4D-shape.\"\"\"\n",
    "        proj_weight = None\n",
    "        if name == 'q':\n",
    "            proj_weight = self.q_proj_weight\n",
    "        elif name == 'k':\n",
    "            proj_weight = self.k_proj_weight\n",
    "        elif name =='v':\n",
    "            proj_weight = self.v_proj_weight\n",
    "        elif name == 'r':\n",
    "            proj_weight = self.r_proj_weight\n",
    "        else:\n",
    "            raise ValueError('Unknown `name` {}.'.format(name))\n",
    "\n",
    "        head = torch.einsum('ibh,hnd->ibnd', h, proj_weight)\n",
    "\n",
    "        return head\n",
    "\n",
    "    def rel_attn_core(self, q_head, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat,\n",
    "                      r_w_bias, r_r_bias, r_s_bias, attn_mask, scale):\n",
    "\n",
    "        \"\"\"Core relative positional attention operations.\"\"\"\n",
    "\n",
    "        # content based attention score\n",
    "        ac = torch.einsum('ibnd,jbnd->ijbn', q_head + r_w_bias, k_head_h)\n",
    "\n",
    "        # position based attention score\n",
    "        bd = torch.einsum('ibnd,jbnd->ijbn', q_head + r_r_bias, k_head_r)\n",
    "        bd = self.rel_shift(bd, klen=ac.shape[1])\n",
    "\n",
    "        # segment based attention score\n",
    "        if seg_mat is None:\n",
    "            ef = 0\n",
    "        else:\n",
    "            ef = torch.einsum('ibnd,snd->ibns', q_head + r_s_bias, seg_embed)\n",
    "            ef = torch.einsum('ijbs,ibns->ijbn', seg_mat, ef)\n",
    "\n",
    "        # merge attention scores and perform masking\n",
    "        attn_score = (ac + bd + ef) * scale\n",
    "        if attn_mask is not None:\n",
    "            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n",
    "            attn_score = attn_score - 1e30 * attn_mask\n",
    "\n",
    "        # attention probability\n",
    "        attn_prob = F.softmax(attn_score, dim=1)\n",
    "        attn_prob = self.DropAttn(attn_prob)\n",
    "\n",
    "        # attention output\n",
    "        attn_vec = torch.einsum('ijbn,jbnd->ibnd', attn_prob, v_head_h)\n",
    "\n",
    "        return attn_vec\n",
    "\n",
    "    def rel_multihead_attn(self, h, r, r_w_bias, r_r_bias, seg_mat, r_s_bias, seg_embed,\n",
    "                           attn_mask, mems, d_model, n_head, d_head, dropout, dropatt):\n",
    "        \"\"\"Multi-head attention with relative positional encoding.\"\"\"\n",
    "\n",
    "        scale = 1 / (d_head ** 0.5)\n",
    "        if mems is not None and len(mems.size()) > 1:\n",
    "            cat = torch.cat([mems, h], dim=0)\n",
    "        else:\n",
    "            cat = h\n",
    "\n",
    "        # content heads\n",
    "        q_head_h = self.head_projection(h, 'q')\n",
    "        k_head_h = self.head_projection(cat, 'k')\n",
    "        v_head_h = self.head_projection(cat, 'v')\n",
    "\n",
    "        # positional heads\n",
    "        k_head_r = self.head_projection(r, 'r')\n",
    "\n",
    "        # core attention ops\n",
    "        attn_vec = self.rel_attn_core(\n",
    "            q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "            r_r_bias, r_s_bias, attn_mask, scale)\n",
    "\n",
    "        # post processing\n",
    "        output = self.post_attention(h, attn_vec)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def two_stream_rel_attn(self, h, g, r, mems, r_w_bias, r_r_bias, seg_mat, r_s_bias,\n",
    "                            seg_embed, attn_mask_h, attn_mask_g, target_mapping):\n",
    "        scale = 1 / (self.d_head ** 0.5)\n",
    "\n",
    "        # content based attention score\n",
    "        if mems is not None and len(mems.size()) > 1:\n",
    "            cat = torch.cat([mems, h], dim=0)\n",
    "        else:\n",
    "            cat = h\n",
    "\n",
    "        # content-based key head\n",
    "        k_head_h = self.head_projection(cat, 'k')\n",
    "\n",
    "        # content-based value head\n",
    "        v_head_h = self.head_projection(cat, 'v')\n",
    "\n",
    "        # position-based key head\n",
    "        k_head_r = self.head_projection(r, 'r')\n",
    "\n",
    "        ##### h-stream\n",
    "        # content-stream query head\n",
    "        q_head_h = self.head_projection(h, 'q')\n",
    "\n",
    "        # core attention ops\n",
    "        # hˆ(m)_zt = LayerNorm\u0010(h^(m-1)_zt + RelAttn(h^(m-1)_zt + [h~^(m-1), hT(m-1)_z<=t]))\n",
    "        attn_vec_h = self.rel_attn_core(\n",
    "            q_head_h, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "            r_r_bias, r_s_bias, attn_mask_h, scale)\n",
    "\n",
    "        # post processing\n",
    "        output_h = self.post_attention(h, attn_vec_h)\n",
    "\n",
    "        ##### g-stream\n",
    "        # query-stream query head\n",
    "        q_head_g = self.head_projection(g, 'q')\n",
    "\n",
    "        # core attention ops\n",
    "        # gˆ(m)_zt = LayerNorm\u0010(g^(m-1)_zt + RelAttn(g^(m-1)_zt + [h~^(m-1), hT(m-1)_z<=t]))\n",
    "        if target_mapping is not None:\n",
    "            q_head_g = torch.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)\n",
    "            attn_vec_g = self.rel_attn_core(\n",
    "                q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "                r_r_bias, r_s_bias, attn_mask_g, scale)\n",
    "            attn_vec_g = torch.einsum('lbnd,mlb->mbnd', attn_vec_g, target_mapping)\n",
    "        else:\n",
    "            attn_vec_g = self.rel_attn_core(\n",
    "                q_head_g, k_head_h, v_head_h, k_head_r, seg_embed, seg_mat, r_w_bias,\n",
    "                r_r_bias, r_s_bias, attn_mask_g, scale)\n",
    "\n",
    "        # post processing\n",
    "        output_g = self.post_attention(g, attn_vec_g)\n",
    "\n",
    "        return output_h, output_g\n",
    "\n",
    "\n",
    "    def _create_mask(self, qlen, mlen, dtype, same_length=False):\n",
    "        \"\"\"create causal attention mask.\"\"\"\n",
    "        # [[0,1,1],\n",
    "        #  [0,0,1],\n",
    "        #  [0,0,0]]\n",
    "        attn_mask = torch.ones([qlen, qlen], dtype=dtype)\n",
    "        mask_u = torch.triu(attn_mask) # Upper triangular part.\n",
    "        mask_dia = torch.tril(attn_mask) & torch.triu(attn_mask) # Diagonal. Figure 2(c)\n",
    "        attn_mask_pad = torch.zeros([qlen, mlen], dtype=dtype)\n",
    "        ret = torch.cat([attn_mask_pad, mask_u - mask_dia], dim=1) # [qlen, mlen]\n",
    "        if same_length:\n",
    "            # [[0,1,1],\n",
    "            #  [1,0,1],\n",
    "            #  [1,1,0]]\n",
    "            mask_l = torch.tril(attn_mask) # Lower triangular part.\n",
    "            ret = torch.cat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], dim=1)\n",
    "\n",
    "        return ret.type(dtype=torch.float32) # [qlen, qlen]\n",
    "\n",
    "    def positional_embedding(self, pos_seq, inv_freq):\n",
    "        sinusoid_inp = torch.einsum('i,d->id', pos_seq, inv_freq)\n",
    "        pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n",
    "        pos_emb = pos_emb[:, None, :]\n",
    "\n",
    "        return pos_emb\n",
    "    \n",
    "    # 현재 세그먼트를 메모리에 저장\n",
    "    def _cache_mem(self, curr_out, prev_mem, mem_len, reuse_len=None):\n",
    "        \"\"\"cache hidden states into memory.\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if mem_len is None or mem_len == 0:\n",
    "                return None\n",
    "            else:\n",
    "                if reuse_len is not None and reuse_len > 0:\n",
    "                    curr_out = curr_out[:reuse_len]\n",
    "\n",
    "                if prev_mem is None:\n",
    "                    new_mem = curr_out[-mem_len:]\n",
    "                else:\n",
    "                    new_mem = torch.cat([prev_mem, curr_out], dim=0)[-mem_len:]\n",
    "\n",
    "            return new_mem\n",
    "\n",
    "\n",
    "    def relative_positional_encoding(self, qlen, klen, d_model, clamp_len, attn_type,\n",
    "                                     bi_data, bsz=None, dtype=None):\n",
    "        \"\"\"create relative positional encoding.\"\"\"\n",
    "\n",
    "        freq_seq = torch.arange(0, d_model, 2.0)\n",
    "        if dtype is not None and dtype != torch.float32:\n",
    "            freq_seq = freq_seq.type(dtype)\n",
    "        inv_freq = 1 / (10000 ** (freq_seq / d_model))\n",
    "\n",
    "        if attn_type == 'bi':\n",
    "            # beg, end = klen - 1, -qlen\n",
    "            beg, end = klen, -qlen\n",
    "        elif attn_type == 'uni':\n",
    "            # beg, end = klen - 1, -1\n",
    "            beg, end = klen, -1\n",
    "        else:\n",
    "            raise ValueError('Unknown `attn_type` {}.'.format(attn_type))\n",
    "\n",
    "        if bi_data and bsz%2 is 0:\n",
    "            fwd_pos_seq = torch.arange(beg, end, -1.0)\n",
    "            bwd_pos_seq = torch.arange(-beg, -end, 1.0)\n",
    "\n",
    "            if dtype is not None and dtype != torch.float32:\n",
    "                fwd_pos_seq = fwd_pos_seq.type(dtype=dtype)\n",
    "                bwd_pos_seq = bwd_pos_seq.type(dtype=dtype)\n",
    "\n",
    "            if clamp_len > 0:\n",
    "                fwd_pos_seq = torch.clamp(fwd_pos_seq, -clamp_len, clamp_len)\n",
    "                bwd_pos_seq = torch.clamp(bwd_pos_seq, -clamp_len, clamp_len)\n",
    "\n",
    "            fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n",
    "            bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n",
    "\n",
    "            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n",
    "        else:\n",
    "            fwd_pos_seq = torch.arange(beg, end, -1.0)\n",
    "            if dtype is not None and dtype != torch.float32:\n",
    "                fwd_pos_seq = fwd_pos_seq.type(dtype=dtype)\n",
    "            if clamp_len > 0:\n",
    "                fwd_pos_seq = torch.clamp(fwd_pos_seq, -clamp_len, clamp_len)\n",
    "            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def forward(self, inp_k, seg_id, input_mask, mems, perm_mask, target_mapping, inp_q):\n",
    "        new_mems = []\n",
    "\n",
    "        bsz = inp_k.shape[1]\n",
    "        qlen = inp_k.shape[0]\n",
    "        mlen = mems[0].size(0) if mems is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        ##### Attention mask\n",
    "        # causal attention mask\n",
    "        if self.attn_type == 'uni':\n",
    "            attn_mask = self._create_mask(qlen, mlen, torch.int64, self.same_length)\n",
    "            attn_mask = attn_mask[:, :, None, None]\n",
    "        elif self.attn_type == 'bi':\n",
    "            attn_mask = None\n",
    "        else:\n",
    "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
    "\n",
    "        # data mask: input mask & perm mask\n",
    "        if input_mask is not None and perm_mask is not None:\n",
    "            data_mask = input_mask[None] + perm_mask\n",
    "        elif input_mask is not None and perm_mask is None:\n",
    "            data_mask = input_mask[None]\n",
    "        elif input_mask is None and perm_mask is not None:\n",
    "            data_mask = perm_mask\n",
    "        else:\n",
    "            data_mask = None\n",
    "\n",
    "        if data_mask is not None:\n",
    "            # all mems can be attended to\n",
    "            mems_mask = torch.zeros([data_mask.shape[0], mlen, bsz],\n",
    "                                 dtype=torch.float32)\n",
    "            data_mask = torch.cat([mems_mask, data_mask], dim=1)\n",
    "            if attn_mask is None:\n",
    "                attn_mask = data_mask[:, :, :, None]\n",
    "            else:\n",
    "                attn_mask += data_mask[:, :, :, None]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.gt(0).type(torch.float32)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            non_tgt_mask = -torch.eye(qlen, dtype=torch.float32) # [qlen, qlen]\n",
    "            non_tgt_mask = torch.cat([torch.zeros([qlen, mlen], dtype=torch.float32), # [qlen, klen]\n",
    "                                        non_tgt_mask],\n",
    "                                        dim=-1)\n",
    "            non_tgt_mask = (attn_mask +\n",
    "                            non_tgt_mask[:, :, None, None]).gt(0).type(dtype=torch.float32)\n",
    "        else:\n",
    "            non_tgt_mask = None\n",
    "\n",
    "        ##### Word embedding\n",
    "        lookup_table = self.embedding\n",
    "        word_emb_k = lookup_table(inp_k)\n",
    "\n",
    "        if inp_q is not None:\n",
    "            if target_mapping is not None:\n",
    "                word_emb_q = self.mask_emb.repeat(target_mapping.shape[0], bsz, 1)\n",
    "            else:\n",
    "                inp_q_ext = inp_q[:, :, None]\n",
    "                word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
    "\n",
    "        #### Figure 2(a), Content Stream(Original Attention), h^(0)_t = e(x_i) = e(inp_k)\n",
    "        output_h = self.Dropout(word_emb_k)\n",
    "        if inp_q is not None:\n",
    "            #### Query Stream, g^(0)_t = w\n",
    "            #### the first layer query stream is initialized with a trainable vector\n",
    "            output_g = self.Dropout(word_emb_q)\n",
    "\n",
    "        ##### Segment embedding\n",
    "        # paper\n",
    "        # Given a pair of positions i and j in the sequence, if\n",
    "        # i and j are from the same segment\n",
    "        if seg_id is not None:\n",
    "            # Convert `seg_id` to one-hot `seg_mat`\n",
    "            mem_pad = torch.zeros([mlen, bsz], dtype=torch.int32)\n",
    "            cat_ids = torch.cat([mem_pad, seg_id], dim=0)\n",
    "\n",
    "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
    "            seg_mat = (~torch.eq(seg_id[:, None], cat_ids[None, :])).type(torch.long)\n",
    "            seg_mat = torch.eye(2, dtype=torch.float32)[seg_mat]\n",
    "        else:\n",
    "            seg_mat = None\n",
    "\n",
    "        ##### Positional encoding\n",
    "        pos_emb = self.relative_positional_encoding(\n",
    "            qlen, klen, self.d_model, self.clamp_len, self.attn_type, self.bi_data,\n",
    "            bsz=bsz, dtype=torch.float32)\n",
    "        pos_emb = self.Dropout(pos_emb)\n",
    "\n",
    "        ##### Attention layers\n",
    "        if mems is None:\n",
    "            mems = [None] * self.n_layer\n",
    "\n",
    "        for i in range(self.n_layer):\n",
    "            # cache new mems\n",
    "            new_mems.append(self._cache_mem(output_h, mems[i], self.mem_len, self.reuse_len))\n",
    "\n",
    "            # segment bias\n",
    "            if seg_id is None:\n",
    "                r_s_bias_i = None\n",
    "                seg_embed_i = None\n",
    "            else:\n",
    "                r_s_bias_i = self.r_s_bias[i]\n",
    "                seg_embed_i = self.seg_embed[i]\n",
    "            \n",
    "            if inp_q is not None:\n",
    "                output_h, output_g = self.two_stream_rel_attn(\n",
    "                    h=output_h,\n",
    "                    g=output_g,\n",
    "                    r=pos_emb,\n",
    "                    r_w_bias= self.r_w_bias[i],\n",
    "                    r_r_bias= self.r_r_bias[i],\n",
    "                    seg_mat=seg_mat,\n",
    "                    r_s_bias=r_s_bias_i,\n",
    "                    seg_embed=seg_embed_i,\n",
    "                    attn_mask_h=non_tgt_mask,\n",
    "                    attn_mask_g=attn_mask,\n",
    "                    mems=mems[i],\n",
    "                    target_mapping=target_mapping)\n",
    "            else:\n",
    "                output_h = self.rel_multihead_attn(\n",
    "                    h=output_h,\n",
    "                    r=pos_emb,\n",
    "                    r_w_bias=self.r_w_bias[i],\n",
    "                    r_r_bias=self.r_r_bias[i],\n",
    "                    seg_mat=seg_mat,\n",
    "                    r_s_bias=r_s_bias_i,\n",
    "                    seg_embed=seg_embed_i,\n",
    "                    attn_mask=non_tgt_mask,\n",
    "                    mems=mems[i])\n",
    "\n",
    "            if inp_q is not None:\n",
    "                output_g = self.positionwise_ffn(inp=output_g)\n",
    "\n",
    "            output_h = self.positionwise_ffn(inp=output_h)\n",
    "\n",
    "        if inp_q is not None:\n",
    "            output = self.Dropout(output_g)\n",
    "        else:\n",
    "            output = self.Dropout(output_h)\n",
    "\n",
    "        logits = torch.einsum('ibd,nd->ibn', output, lookup_table.weight) + self.softmax_b\n",
    "\n",
    "        return logits, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b838984",
   "metadata": {},
   "source": [
    "#### toeknizer 및 dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b78d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch_pretrained_bert 주석 해제하여 사전 설치 필요\n",
    "# !pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4dcf17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:04:59.888175Z",
     "start_time": "2022-12-18T18:04:58.584169Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bert에서 사용된 Tokenizer 그대로 활용\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# dict로 각 text(갯수 : 30522)에 따른 index 부여되어 있음\n",
    "sp = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61580093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:04:59.903651Z",
     "start_time": "2022-12-18T18:04:59.890153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38123484",
   "metadata": {},
   "source": [
    "#### XLNet class 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d7bb4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:05:01.210738Z",
     "start_time": "2022-12-18T18:05:01.191500Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XLNet(n_token=len(sp.vocab), n_layer=6, n_head=4, d_head=8,\n",
    "              d_inner=32, d_model=32,\n",
    "              dropout=0.1, dropatt=0.1,\n",
    "              attn_type=\"bi\", bi_data=False,\n",
    "              clamp_len=-1, same_length=False,\n",
    "              reuse_len=256, mem_len=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d420a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:05:02.103936Z",
     "start_time": "2022-12-18T18:05:02.088827Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of XLNet(\n",
       "  (embedding): Embedding(30522, 32)\n",
       "  (Dropout): Dropout(p=0.1, inplace=False)\n",
       "  (DropAttn): Dropout(p=0.1, inplace=False)\n",
       "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (conv2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f41a0",
   "metadata": {},
   "source": [
    "We can check the number of parameters, noticing it is significantly less than the 37M for the convolutional sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54ae6549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:32:30.369540Z",
     "start_time": "2022-12-18T18:32:30.359539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,015,514 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff34e1",
   "metadata": {},
   "source": [
    "#### optimization 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ae1956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:05:03.812861Z",
     "start_time": "2022-12-18T18:05:03.799296Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0473e55",
   "metadata": {},
   "source": [
    "#### train 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c81639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:05:57.317983Z",
     "start_time": "2022-12-18T18:05:57.303486Z"
    }
   },
   "outputs": [],
   "source": [
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59e8ebf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:16:52.666832Z",
     "start_time": "2022-12-18T18:16:52.658831Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "data_path = 'data.txt'        # data.txt's 경로\n",
    "seq_len = 512                 # Sequence length.\n",
    "reuse_len = 256               # \"Number of token that can be reused as memory. Could be half of `seq_len`.\"\n",
    "bi_data = False               # whether to create bidirectional data\n",
    "num_predict = 85              # Num of tokens to predict.\n",
    "mask_alpha = 6                # How many tokens to form a group.\n",
    "mask_beta = 1                 # How many tokens to mask within each group.\n",
    "perm_size = 256               # the length of longest permutation. Could be set to be reuse_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a26bf88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:08:30.846938Z",
     "start_time": "2022-12-18T18:08:30.782027Z"
    }
   },
   "outputs": [],
   "source": [
    "features = data_utils._create_data(sp=sp,\n",
    "                                   input_paths=data_path,\n",
    "                                   seq_len=seq_len,\n",
    "                                   reuse_len=reuse_len,\n",
    "                                   bi_data=bi_data,\n",
    "                                   num_predict=num_predict,\n",
    "                                   mask_alpha=mask_alpha,\n",
    "                                   mask_beta=mask_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b1305f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:08:01.806988Z",
     "start_time": "2022-12-18T18:08:01.803488Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"data_utils._create_data\" 로부터 학습을 위한 데이터셋으로 처리되는 것을 확인할 수 있음\n",
    "# features[0]['input']\n",
    "# features[0]['is_masked']\n",
    "# features[0]['target']\n",
    "# features[0]['seg_id']\n",
    "# features[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76b8207a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T18:28:38.038477Z",
     "start_time": "2022-12-18T18:16:54.987304Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lhj91\\Git\\SGU_2022_NLP\\XLNet\\data_utils.py:302: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:402.)\n",
      "  rev_index = torch.where(non_mask_tokens, smallest_index, index)\n",
      "C:\\Users\\lhj91\\Git\\SGU_2022_NLP\\XLNet\\data_utils.py:368: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  indices = indices[bool_target_mask]\n",
      "C:\\Users\\lhj91\\Git\\SGU_2022_NLP\\XLNet\\data_utils.py:383: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  target = target[bool_target_mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch: 0001 in 0001 Step cost = 25.014235\n",
      "Number of Epoch: 0001 in 0002 Step cost = 25.571665\n",
      "Number of Epoch: 0001 in 0003 Step cost = 24.997086\n",
      "Number of Epoch: 0001 in 0004 Step cost = 24.105928\n",
      "Number of Epoch: 0001 in 0005 Step cost = 25.697880\n",
      "Number of Epoch: 0001 in 0006 Step cost = 25.271872\n",
      "Number of Epoch: 0001 in 0007 Step cost = 23.993153\n",
      "Number of Epoch: 0001 in 0008 Step cost = 24.123852\n",
      "Number of Epoch: 0002 in 0001 Step cost = 24.452236\n",
      "Number of Epoch: 0002 in 0002 Step cost = 25.485203\n",
      "Number of Epoch: 0002 in 0003 Step cost = 25.462999\n",
      "Number of Epoch: 0002 in 0004 Step cost = 24.514482\n",
      "Number of Epoch: 0002 in 0005 Step cost = 26.535751\n",
      "Number of Epoch: 0002 in 0006 Step cost = 24.345350\n",
      "Number of Epoch: 0002 in 0007 Step cost = 23.777176\n",
      "Number of Epoch: 0002 in 0008 Step cost = 25.253572\n",
      "Number of Epoch: 0003 in 0001 Step cost = 25.282433\n",
      "Number of Epoch: 0003 in 0002 Step cost = 24.759079\n",
      "Number of Epoch: 0003 in 0003 Step cost = 25.388359\n",
      "Number of Epoch: 0003 in 0004 Step cost = 25.001631\n",
      "Number of Epoch: 0003 in 0005 Step cost = 25.001562\n",
      "Number of Epoch: 0003 in 0006 Step cost = 25.509727\n",
      "Number of Epoch: 0003 in 0007 Step cost = 24.215456\n",
      "Number of Epoch: 0003 in 0008 Step cost = 26.457157\n",
      "Number of Epoch: 0004 in 0001 Step cost = 25.586740\n",
      "Number of Epoch: 0004 in 0002 Step cost = 25.016245\n",
      "Number of Epoch: 0004 in 0003 Step cost = 23.690866\n",
      "Number of Epoch: 0004 in 0004 Step cost = 24.316906\n",
      "Number of Epoch: 0004 in 0005 Step cost = 25.173672\n",
      "Number of Epoch: 0004 in 0006 Step cost = 26.038334\n",
      "Number of Epoch: 0004 in 0007 Step cost = 25.412571\n",
      "Number of Epoch: 0004 in 0008 Step cost = 26.344387\n",
      "Number of Epoch: 0005 in 0001 Step cost = 25.065269\n",
      "Number of Epoch: 0005 in 0002 Step cost = 24.134037\n",
      "Number of Epoch: 0005 in 0003 Step cost = 24.082079\n",
      "Number of Epoch: 0005 in 0004 Step cost = 24.401329\n",
      "Number of Epoch: 0005 in 0005 Step cost = 25.398685\n",
      "Number of Epoch: 0005 in 0006 Step cost = 25.875011\n",
      "Number of Epoch: 0005 in 0007 Step cost = 24.627089\n",
      "Number of Epoch: 0005 in 0008 Step cost = 24.722832\n",
      "Number of Epoch: 0006 in 0001 Step cost = 23.462440\n",
      "Number of Epoch: 0006 in 0002 Step cost = 24.355888\n",
      "Number of Epoch: 0006 in 0003 Step cost = 25.281839\n",
      "Number of Epoch: 0006 in 0004 Step cost = 24.511850\n",
      "Number of Epoch: 0006 in 0005 Step cost = 24.610518\n",
      "Number of Epoch: 0006 in 0006 Step cost = 24.030056\n",
      "Number of Epoch: 0006 in 0007 Step cost = 25.042845\n",
      "Number of Epoch: 0006 in 0008 Step cost = 25.702671\n",
      "Number of Epoch: 0007 in 0001 Step cost = 23.664711\n",
      "Number of Epoch: 0007 in 0002 Step cost = 23.811012\n",
      "Number of Epoch: 0007 in 0003 Step cost = 23.016151\n",
      "Number of Epoch: 0007 in 0004 Step cost = 24.042772\n",
      "Number of Epoch: 0007 in 0005 Step cost = 24.522387\n",
      "Number of Epoch: 0007 in 0006 Step cost = 25.847141\n",
      "Number of Epoch: 0007 in 0007 Step cost = 24.744967\n",
      "Number of Epoch: 0007 in 0008 Step cost = 25.380692\n",
      "Number of Epoch: 0008 in 0001 Step cost = 24.470285\n",
      "Number of Epoch: 0008 in 0002 Step cost = 24.192793\n",
      "Number of Epoch: 0008 in 0003 Step cost = 24.449890\n",
      "Number of Epoch: 0008 in 0004 Step cost = 24.042282\n",
      "Number of Epoch: 0008 in 0005 Step cost = 24.326462\n",
      "Number of Epoch: 0008 in 0006 Step cost = 23.669949\n",
      "Number of Epoch: 0008 in 0007 Step cost = 24.985779\n",
      "Number of Epoch: 0008 in 0008 Step cost = 23.577343\n",
      "Number of Epoch: 0009 in 0001 Step cost = 24.369486\n",
      "Number of Epoch: 0009 in 0002 Step cost = 24.110682\n",
      "Number of Epoch: 0009 in 0003 Step cost = 23.930418\n",
      "Number of Epoch: 0009 in 0004 Step cost = 23.896338\n",
      "Number of Epoch: 0009 in 0005 Step cost = 24.474842\n",
      "Number of Epoch: 0009 in 0006 Step cost = 24.042349\n",
      "Number of Epoch: 0009 in 0007 Step cost = 23.150986\n",
      "Number of Epoch: 0009 in 0008 Step cost = 24.176113\n",
      "Number of Epoch: 0010 in 0001 Step cost = 22.767458\n",
      "Number of Epoch: 0010 in 0002 Step cost = 23.860435\n",
      "Number of Epoch: 0010 in 0003 Step cost = 25.413137\n",
      "Number of Epoch: 0010 in 0004 Step cost = 24.540749\n",
      "Number of Epoch: 0010 in 0005 Step cost = 24.630386\n",
      "Number of Epoch: 0010 in 0006 Step cost = 22.770473\n",
      "Number of Epoch: 0010 in 0007 Step cost = 25.157570\n",
      "Number of Epoch: 0010 in 0008 Step cost = 25.540806\n",
      "Number of Epoch: 0011 in 0001 Step cost = 23.316811\n",
      "Number of Epoch: 0011 in 0002 Step cost = 24.225683\n",
      "Number of Epoch: 0011 in 0003 Step cost = 23.400867\n",
      "Number of Epoch: 0011 in 0004 Step cost = 22.708239\n",
      "Number of Epoch: 0011 in 0005 Step cost = 24.957668\n",
      "Number of Epoch: 0011 in 0006 Step cost = 24.616871\n",
      "Number of Epoch: 0011 in 0007 Step cost = 24.723667\n",
      "Number of Epoch: 0011 in 0008 Step cost = 25.318386\n",
      "Number of Epoch: 0012 in 0001 Step cost = 24.426746\n",
      "Number of Epoch: 0012 in 0002 Step cost = 23.852451\n",
      "Number of Epoch: 0012 in 0003 Step cost = 23.396923\n",
      "Number of Epoch: 0012 in 0004 Step cost = 23.583632\n",
      "Number of Epoch: 0012 in 0005 Step cost = 22.923512\n",
      "Number of Epoch: 0012 in 0006 Step cost = 24.487553\n",
      "Number of Epoch: 0012 in 0007 Step cost = 24.349478\n",
      "Number of Epoch: 0012 in 0008 Step cost = 24.678787\n",
      "Number of Epoch: 0013 in 0001 Step cost = 24.356554\n",
      "Number of Epoch: 0013 in 0002 Step cost = 23.912796\n",
      "Number of Epoch: 0013 in 0003 Step cost = 25.377213\n",
      "Number of Epoch: 0013 in 0004 Step cost = 23.891930\n",
      "Number of Epoch: 0013 in 0005 Step cost = 24.282211\n",
      "Number of Epoch: 0013 in 0006 Step cost = 23.836781\n",
      "Number of Epoch: 0013 in 0007 Step cost = 23.566675\n",
      "Number of Epoch: 0013 in 0008 Step cost = 24.124380\n",
      "Number of Epoch: 0014 in 0001 Step cost = 24.681040\n",
      "Number of Epoch: 0014 in 0002 Step cost = 23.041115\n",
      "Number of Epoch: 0014 in 0003 Step cost = 24.569641\n",
      "Number of Epoch: 0014 in 0004 Step cost = 23.351364\n",
      "Number of Epoch: 0014 in 0005 Step cost = 23.451021\n",
      "Number of Epoch: 0014 in 0006 Step cost = 22.884933\n",
      "Number of Epoch: 0014 in 0007 Step cost = 23.297110\n",
      "Number of Epoch: 0014 in 0008 Step cost = 23.321226\n",
      "Number of Epoch: 0015 in 0001 Step cost = 22.959518\n",
      "Number of Epoch: 0015 in 0002 Step cost = 23.709152\n",
      "Number of Epoch: 0015 in 0003 Step cost = 22.442696\n",
      "Number of Epoch: 0015 in 0004 Step cost = 23.396757\n",
      "Number of Epoch: 0015 in 0005 Step cost = 24.065845\n",
      "Number of Epoch: 0015 in 0006 Step cost = 23.246170\n",
      "Number of Epoch: 0015 in 0007 Step cost = 23.803104\n",
      "Number of Epoch: 0015 in 0008 Step cost = 23.733458\n",
      "Number of Epoch: 0016 in 0001 Step cost = 22.990980\n",
      "Number of Epoch: 0016 in 0002 Step cost = 24.825354\n",
      "Number of Epoch: 0016 in 0003 Step cost = 23.460154\n",
      "Number of Epoch: 0016 in 0004 Step cost = 24.279837\n",
      "Number of Epoch: 0016 in 0005 Step cost = 24.615099\n",
      "Number of Epoch: 0016 in 0006 Step cost = 24.619192\n",
      "Number of Epoch: 0016 in 0007 Step cost = 23.872475\n",
      "Number of Epoch: 0016 in 0008 Step cost = 22.027662\n",
      "Number of Epoch: 0017 in 0001 Step cost = 23.878561\n",
      "Number of Epoch: 0017 in 0002 Step cost = 23.229483\n",
      "Number of Epoch: 0017 in 0003 Step cost = 22.319616\n",
      "Number of Epoch: 0017 in 0004 Step cost = 24.555635\n",
      "Number of Epoch: 0017 in 0005 Step cost = 23.746656\n",
      "Number of Epoch: 0017 in 0006 Step cost = 23.272360\n",
      "Number of Epoch: 0017 in 0007 Step cost = 24.120798\n",
      "Number of Epoch: 0017 in 0008 Step cost = 23.786827\n",
      "Number of Epoch: 0018 in 0001 Step cost = 22.969835\n",
      "Number of Epoch: 0018 in 0002 Step cost = 23.274366\n",
      "Number of Epoch: 0018 in 0003 Step cost = 24.127333\n",
      "Number of Epoch: 0018 in 0004 Step cost = 23.489660\n",
      "Number of Epoch: 0018 in 0005 Step cost = 22.626892\n",
      "Number of Epoch: 0018 in 0006 Step cost = 23.758070\n",
      "Number of Epoch: 0018 in 0007 Step cost = 23.896170\n",
      "Number of Epoch: 0018 in 0008 Step cost = 22.255587\n",
      "Number of Epoch: 0019 in 0001 Step cost = 22.804226\n",
      "Number of Epoch: 0019 in 0002 Step cost = 22.068161\n",
      "Number of Epoch: 0019 in 0003 Step cost = 22.664944\n",
      "Number of Epoch: 0019 in 0004 Step cost = 23.973940\n",
      "Number of Epoch: 0019 in 0005 Step cost = 23.685486\n",
      "Number of Epoch: 0019 in 0006 Step cost = 23.180759\n",
      "Number of Epoch: 0019 in 0007 Step cost = 23.231350\n",
      "Number of Epoch: 0019 in 0008 Step cost = 22.827688\n",
      "Number of Epoch: 0020 in 0001 Step cost = 22.966393\n",
      "Number of Epoch: 0020 in 0002 Step cost = 23.695944\n",
      "Number of Epoch: 0020 in 0003 Step cost = 22.922668\n",
      "Number of Epoch: 0020 in 0004 Step cost = 23.633263\n",
      "Number of Epoch: 0020 in 0005 Step cost = 22.203970\n",
      "Number of Epoch: 0020 in 0006 Step cost = 23.064207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch: 0020 in 0007 Step cost = 23.133493\n",
      "Number of Epoch: 0020 in 0008 Step cost = 24.223341\n",
      "Number of Epoch: 0021 in 0001 Step cost = 22.765358\n",
      "Number of Epoch: 0021 in 0002 Step cost = 23.647488\n",
      "Number of Epoch: 0021 in 0003 Step cost = 23.032467\n",
      "Number of Epoch: 0021 in 0004 Step cost = 22.854006\n",
      "Number of Epoch: 0021 in 0005 Step cost = 23.918406\n",
      "Number of Epoch: 0021 in 0006 Step cost = 24.545650\n",
      "Number of Epoch: 0021 in 0007 Step cost = 23.378416\n",
      "Number of Epoch: 0021 in 0008 Step cost = 21.982777\n",
      "Number of Epoch: 0022 in 0001 Step cost = 23.657215\n",
      "Number of Epoch: 0022 in 0002 Step cost = 24.234209\n",
      "Number of Epoch: 0022 in 0003 Step cost = 22.743130\n",
      "Number of Epoch: 0022 in 0004 Step cost = 22.302784\n",
      "Number of Epoch: 0022 in 0005 Step cost = 21.919886\n",
      "Number of Epoch: 0022 in 0006 Step cost = 21.981691\n",
      "Number of Epoch: 0022 in 0007 Step cost = 21.613743\n",
      "Number of Epoch: 0022 in 0008 Step cost = 23.124096\n",
      "Number of Epoch: 0023 in 0001 Step cost = 22.184471\n",
      "Number of Epoch: 0023 in 0002 Step cost = 23.135654\n",
      "Number of Epoch: 0023 in 0003 Step cost = 22.891821\n",
      "Number of Epoch: 0023 in 0004 Step cost = 23.688648\n",
      "Number of Epoch: 0023 in 0005 Step cost = 22.041170\n",
      "Number of Epoch: 0023 in 0006 Step cost = 23.441027\n",
      "Number of Epoch: 0023 in 0007 Step cost = 22.857933\n",
      "Number of Epoch: 0023 in 0008 Step cost = 23.407578\n",
      "Number of Epoch: 0024 in 0001 Step cost = 23.735924\n",
      "Number of Epoch: 0024 in 0002 Step cost = 23.835526\n",
      "Number of Epoch: 0024 in 0003 Step cost = 23.512827\n",
      "Number of Epoch: 0024 in 0004 Step cost = 23.556017\n",
      "Number of Epoch: 0024 in 0005 Step cost = 22.858641\n",
      "Number of Epoch: 0024 in 0006 Step cost = 23.099277\n",
      "Number of Epoch: 0024 in 0007 Step cost = 22.559202\n",
      "Number of Epoch: 0024 in 0008 Step cost = 22.213469\n",
      "Number of Epoch: 0025 in 0001 Step cost = 22.392347\n",
      "Number of Epoch: 0025 in 0002 Step cost = 21.890587\n",
      "Number of Epoch: 0025 in 0003 Step cost = 23.096973\n",
      "Number of Epoch: 0025 in 0004 Step cost = 22.443241\n",
      "Number of Epoch: 0025 in 0005 Step cost = 23.037327\n",
      "Number of Epoch: 0025 in 0006 Step cost = 22.583975\n",
      "Number of Epoch: 0025 in 0007 Step cost = 21.669239\n",
      "Number of Epoch: 0025 in 0008 Step cost = 23.392258\n",
      "Number of Epoch: 0026 in 0001 Step cost = 20.546877\n",
      "Number of Epoch: 0026 in 0002 Step cost = 22.188108\n",
      "Number of Epoch: 0026 in 0003 Step cost = 20.569153\n",
      "Number of Epoch: 0026 in 0004 Step cost = 22.294680\n",
      "Number of Epoch: 0026 in 0005 Step cost = 23.252998\n",
      "Number of Epoch: 0026 in 0006 Step cost = 20.930969\n",
      "Number of Epoch: 0026 in 0007 Step cost = 22.622644\n",
      "Number of Epoch: 0026 in 0008 Step cost = 22.732580\n",
      "Number of Epoch: 0027 in 0001 Step cost = 21.596487\n",
      "Number of Epoch: 0027 in 0002 Step cost = 22.058395\n",
      "Number of Epoch: 0027 in 0003 Step cost = 21.991623\n",
      "Number of Epoch: 0027 in 0004 Step cost = 22.318632\n",
      "Number of Epoch: 0027 in 0005 Step cost = 22.911669\n",
      "Number of Epoch: 0027 in 0006 Step cost = 21.947044\n",
      "Number of Epoch: 0027 in 0007 Step cost = 22.130175\n",
      "Number of Epoch: 0027 in 0008 Step cost = 22.941601\n",
      "Number of Epoch: 0028 in 0001 Step cost = 22.087915\n",
      "Number of Epoch: 0028 in 0002 Step cost = 20.697853\n",
      "Number of Epoch: 0028 in 0003 Step cost = 22.825731\n",
      "Number of Epoch: 0028 in 0004 Step cost = 23.540419\n",
      "Number of Epoch: 0028 in 0005 Step cost = 24.004515\n",
      "Number of Epoch: 0028 in 0006 Step cost = 22.450697\n",
      "Number of Epoch: 0028 in 0007 Step cost = 22.634705\n",
      "Number of Epoch: 0028 in 0008 Step cost = 22.720823\n",
      "Number of Epoch: 0029 in 0001 Step cost = 21.780287\n",
      "Number of Epoch: 0029 in 0002 Step cost = 22.997547\n",
      "Number of Epoch: 0029 in 0003 Step cost = 22.113537\n",
      "Number of Epoch: 0029 in 0004 Step cost = 23.047926\n",
      "Number of Epoch: 0029 in 0005 Step cost = 22.610863\n",
      "Number of Epoch: 0029 in 0006 Step cost = 22.921289\n",
      "Number of Epoch: 0029 in 0007 Step cost = 22.032677\n",
      "Number of Epoch: 0029 in 0008 Step cost = 22.188650\n",
      "Number of Epoch: 0030 in 0001 Step cost = 21.921728\n",
      "Number of Epoch: 0030 in 0002 Step cost = 23.271875\n",
      "Number of Epoch: 0030 in 0003 Step cost = 22.607534\n",
      "Number of Epoch: 0030 in 0004 Step cost = 21.343590\n",
      "Number of Epoch: 0030 in 0005 Step cost = 21.915844\n",
      "Number of Epoch: 0030 in 0006 Step cost = 22.515821\n",
      "Number of Epoch: 0030 in 0007 Step cost = 21.802475\n",
      "Number of Epoch: 0030 in 0008 Step cost = 22.218163\n",
      "Number of Epoch: 0031 in 0001 Step cost = 21.729343\n",
      "Number of Epoch: 0031 in 0002 Step cost = 21.731577\n",
      "Number of Epoch: 0031 in 0003 Step cost = 22.930763\n",
      "Number of Epoch: 0031 in 0004 Step cost = 21.185329\n",
      "Number of Epoch: 0031 in 0005 Step cost = 21.332108\n",
      "Number of Epoch: 0031 in 0006 Step cost = 22.898184\n",
      "Number of Epoch: 0031 in 0007 Step cost = 21.560478\n",
      "Number of Epoch: 0031 in 0008 Step cost = 21.412508\n",
      "Number of Epoch: 0032 in 0001 Step cost = 21.431923\n",
      "Number of Epoch: 0032 in 0002 Step cost = 21.972898\n",
      "Number of Epoch: 0032 in 0003 Step cost = 22.432518\n",
      "Number of Epoch: 0032 in 0004 Step cost = 21.316885\n",
      "Number of Epoch: 0032 in 0005 Step cost = 23.353041\n",
      "Number of Epoch: 0032 in 0006 Step cost = 22.661438\n",
      "Number of Epoch: 0032 in 0007 Step cost = 21.558971\n",
      "Number of Epoch: 0032 in 0008 Step cost = 21.733444\n",
      "Number of Epoch: 0033 in 0001 Step cost = 21.903561\n",
      "Number of Epoch: 0033 in 0002 Step cost = 22.892248\n",
      "Number of Epoch: 0033 in 0003 Step cost = 21.542349\n",
      "Number of Epoch: 0033 in 0004 Step cost = 22.345299\n",
      "Number of Epoch: 0033 in 0005 Step cost = 22.473276\n",
      "Number of Epoch: 0033 in 0006 Step cost = 20.139259\n",
      "Number of Epoch: 0033 in 0007 Step cost = 21.732616\n",
      "Number of Epoch: 0033 in 0008 Step cost = 22.171873\n",
      "Number of Epoch: 0034 in 0001 Step cost = 22.004417\n",
      "Number of Epoch: 0034 in 0002 Step cost = 22.425772\n",
      "Number of Epoch: 0034 in 0003 Step cost = 20.154758\n",
      "Number of Epoch: 0034 in 0004 Step cost = 21.371128\n",
      "Number of Epoch: 0034 in 0005 Step cost = 20.883337\n",
      "Number of Epoch: 0034 in 0006 Step cost = 22.496935\n",
      "Number of Epoch: 0034 in 0007 Step cost = 20.997097\n",
      "Number of Epoch: 0034 in 0008 Step cost = 22.193871\n",
      "Number of Epoch: 0035 in 0001 Step cost = 20.927353\n",
      "Number of Epoch: 0035 in 0002 Step cost = 21.583206\n",
      "Number of Epoch: 0035 in 0003 Step cost = 21.547850\n",
      "Number of Epoch: 0035 in 0004 Step cost = 21.748537\n",
      "Number of Epoch: 0035 in 0005 Step cost = 20.910818\n",
      "Number of Epoch: 0035 in 0006 Step cost = 21.935585\n",
      "Number of Epoch: 0035 in 0007 Step cost = 21.794680\n",
      "Number of Epoch: 0035 in 0008 Step cost = 20.709549\n",
      "Number of Epoch: 0036 in 0001 Step cost = 21.313707\n",
      "Number of Epoch: 0036 in 0002 Step cost = 20.527416\n",
      "Number of Epoch: 0036 in 0003 Step cost = 22.313353\n",
      "Number of Epoch: 0036 in 0004 Step cost = 21.022083\n",
      "Number of Epoch: 0036 in 0005 Step cost = 20.369358\n",
      "Number of Epoch: 0036 in 0006 Step cost = 21.651707\n",
      "Number of Epoch: 0036 in 0007 Step cost = 20.556980\n",
      "Number of Epoch: 0036 in 0008 Step cost = 21.178635\n",
      "Number of Epoch: 0037 in 0001 Step cost = 22.273510\n",
      "Number of Epoch: 0037 in 0002 Step cost = 20.226074\n",
      "Number of Epoch: 0037 in 0003 Step cost = 21.172215\n",
      "Number of Epoch: 0037 in 0004 Step cost = 21.377062\n",
      "Number of Epoch: 0037 in 0005 Step cost = 19.921593\n",
      "Number of Epoch: 0037 in 0006 Step cost = 21.559128\n",
      "Number of Epoch: 0037 in 0007 Step cost = 21.751709\n",
      "Number of Epoch: 0037 in 0008 Step cost = 21.343468\n",
      "Number of Epoch: 0038 in 0001 Step cost = 22.367502\n",
      "Number of Epoch: 0038 in 0002 Step cost = 21.870529\n",
      "Number of Epoch: 0038 in 0003 Step cost = 21.504641\n",
      "Number of Epoch: 0038 in 0004 Step cost = 20.287657\n",
      "Number of Epoch: 0038 in 0005 Step cost = 20.873386\n",
      "Number of Epoch: 0038 in 0006 Step cost = 21.644396\n",
      "Number of Epoch: 0038 in 0007 Step cost = 20.886019\n",
      "Number of Epoch: 0038 in 0008 Step cost = 20.496702\n",
      "Number of Epoch: 0039 in 0001 Step cost = 21.291502\n",
      "Number of Epoch: 0039 in 0002 Step cost = 20.342957\n",
      "Number of Epoch: 0039 in 0003 Step cost = 21.963099\n",
      "Number of Epoch: 0039 in 0004 Step cost = 20.882801\n",
      "Number of Epoch: 0039 in 0005 Step cost = 21.365662\n",
      "Number of Epoch: 0039 in 0006 Step cost = 21.717710\n",
      "Number of Epoch: 0039 in 0007 Step cost = 20.991726\n",
      "Number of Epoch: 0039 in 0008 Step cost = 21.808504\n",
      "Number of Epoch: 0040 in 0001 Step cost = 21.741507\n",
      "Number of Epoch: 0040 in 0002 Step cost = 22.127342\n",
      "Number of Epoch: 0040 in 0003 Step cost = 21.396801\n",
      "Number of Epoch: 0040 in 0004 Step cost = 21.528587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch: 0040 in 0005 Step cost = 20.519447\n",
      "Number of Epoch: 0040 in 0006 Step cost = 21.087219\n",
      "Number of Epoch: 0040 in 0007 Step cost = 20.251898\n",
      "Number of Epoch: 0040 in 0008 Step cost = 22.531727\n",
      "Number of Epoch: 0041 in 0001 Step cost = 21.294716\n",
      "Number of Epoch: 0041 in 0002 Step cost = 21.603745\n",
      "Number of Epoch: 0041 in 0003 Step cost = 21.050819\n",
      "Number of Epoch: 0041 in 0004 Step cost = 20.195625\n",
      "Number of Epoch: 0041 in 0005 Step cost = 20.000521\n",
      "Number of Epoch: 0041 in 0006 Step cost = 21.335209\n",
      "Number of Epoch: 0041 in 0007 Step cost = 22.252846\n",
      "Number of Epoch: 0041 in 0008 Step cost = 20.205215\n",
      "Number of Epoch: 0042 in 0001 Step cost = 19.827087\n",
      "Number of Epoch: 0042 in 0002 Step cost = 20.958473\n",
      "Number of Epoch: 0042 in 0003 Step cost = 19.785349\n",
      "Number of Epoch: 0042 in 0004 Step cost = 21.854544\n",
      "Number of Epoch: 0042 in 0005 Step cost = 20.272028\n",
      "Number of Epoch: 0042 in 0006 Step cost = 21.257263\n",
      "Number of Epoch: 0042 in 0007 Step cost = 20.541697\n",
      "Number of Epoch: 0042 in 0008 Step cost = 20.788757\n",
      "Number of Epoch: 0043 in 0001 Step cost = 20.423279\n",
      "Number of Epoch: 0043 in 0002 Step cost = 21.453604\n",
      "Number of Epoch: 0043 in 0003 Step cost = 20.861418\n",
      "Number of Epoch: 0043 in 0004 Step cost = 20.271841\n",
      "Number of Epoch: 0043 in 0005 Step cost = 20.872997\n",
      "Number of Epoch: 0043 in 0006 Step cost = 21.345732\n",
      "Number of Epoch: 0043 in 0007 Step cost = 20.431154\n",
      "Number of Epoch: 0043 in 0008 Step cost = 20.257019\n",
      "Number of Epoch: 0044 in 0001 Step cost = 20.207773\n",
      "Number of Epoch: 0044 in 0002 Step cost = 18.978773\n",
      "Number of Epoch: 0044 in 0003 Step cost = 20.191250\n",
      "Number of Epoch: 0044 in 0004 Step cost = 20.798437\n",
      "Number of Epoch: 0044 in 0005 Step cost = 20.813271\n",
      "Number of Epoch: 0044 in 0006 Step cost = 20.998085\n",
      "Number of Epoch: 0044 in 0007 Step cost = 20.046494\n",
      "Number of Epoch: 0044 in 0008 Step cost = 20.693285\n",
      "Number of Epoch: 0045 in 0001 Step cost = 21.052095\n",
      "Number of Epoch: 0045 in 0002 Step cost = 21.051096\n",
      "Number of Epoch: 0045 in 0003 Step cost = 20.819359\n",
      "Number of Epoch: 0045 in 0004 Step cost = 21.522730\n",
      "Number of Epoch: 0045 in 0005 Step cost = 20.710854\n",
      "Number of Epoch: 0045 in 0006 Step cost = 20.462875\n",
      "Number of Epoch: 0045 in 0007 Step cost = 20.455721\n",
      "Number of Epoch: 0045 in 0008 Step cost = 19.544741\n",
      "Number of Epoch: 0046 in 0001 Step cost = 20.135643\n",
      "Number of Epoch: 0046 in 0002 Step cost = 20.347607\n",
      "Number of Epoch: 0046 in 0003 Step cost = 20.203348\n",
      "Number of Epoch: 0046 in 0004 Step cost = 20.672474\n",
      "Number of Epoch: 0046 in 0005 Step cost = 20.564625\n",
      "Number of Epoch: 0046 in 0006 Step cost = 19.272886\n",
      "Number of Epoch: 0046 in 0007 Step cost = 20.708473\n",
      "Number of Epoch: 0046 in 0008 Step cost = 19.942171\n",
      "Number of Epoch: 0047 in 0001 Step cost = 21.042210\n",
      "Number of Epoch: 0047 in 0002 Step cost = 21.129013\n",
      "Number of Epoch: 0047 in 0003 Step cost = 20.582840\n",
      "Number of Epoch: 0047 in 0004 Step cost = 19.788366\n",
      "Number of Epoch: 0047 in 0005 Step cost = 21.957592\n",
      "Number of Epoch: 0047 in 0006 Step cost = 21.231577\n",
      "Number of Epoch: 0047 in 0007 Step cost = 21.810598\n",
      "Number of Epoch: 0047 in 0008 Step cost = 20.012419\n",
      "Number of Epoch: 0048 in 0001 Step cost = 20.524181\n",
      "Number of Epoch: 0048 in 0002 Step cost = 19.913101\n",
      "Number of Epoch: 0048 in 0003 Step cost = 20.410545\n",
      "Number of Epoch: 0048 in 0004 Step cost = 20.278330\n",
      "Number of Epoch: 0048 in 0005 Step cost = 19.752432\n",
      "Number of Epoch: 0048 in 0006 Step cost = 21.427931\n",
      "Number of Epoch: 0048 in 0007 Step cost = 19.154665\n",
      "Number of Epoch: 0048 in 0008 Step cost = 20.561644\n",
      "Number of Epoch: 0049 in 0001 Step cost = 20.214224\n",
      "Number of Epoch: 0049 in 0002 Step cost = 20.336622\n",
      "Number of Epoch: 0049 in 0003 Step cost = 19.619505\n",
      "Number of Epoch: 0049 in 0004 Step cost = 19.487099\n",
      "Number of Epoch: 0049 in 0005 Step cost = 20.029318\n",
      "Number of Epoch: 0049 in 0006 Step cost = 18.525112\n",
      "Number of Epoch: 0049 in 0007 Step cost = 21.288000\n",
      "Number of Epoch: 0049 in 0008 Step cost = 20.161333\n",
      "Number of Epoch: 0050 in 0001 Step cost = 20.631077\n",
      "Number of Epoch: 0050 in 0002 Step cost = 19.477793\n",
      "Number of Epoch: 0050 in 0003 Step cost = 20.019533\n",
      "Number of Epoch: 0050 in 0004 Step cost = 18.933008\n",
      "Number of Epoch: 0050 in 0005 Step cost = 20.034887\n",
      "Number of Epoch: 0050 in 0006 Step cost = 20.808506\n",
      "Number of Epoch: 0050 in 0007 Step cost = 20.215248\n",
      "Number of Epoch: 0050 in 0008 Step cost = 19.265905\n",
      "Number of Epoch: 0051 in 0001 Step cost = 19.305490\n",
      "Number of Epoch: 0051 in 0002 Step cost = 20.775572\n",
      "Number of Epoch: 0051 in 0003 Step cost = 18.504526\n",
      "Number of Epoch: 0051 in 0004 Step cost = 19.382006\n",
      "Number of Epoch: 0051 in 0005 Step cost = 19.164211\n",
      "Number of Epoch: 0051 in 0006 Step cost = 19.590904\n",
      "Number of Epoch: 0051 in 0007 Step cost = 19.098778\n",
      "Number of Epoch: 0051 in 0008 Step cost = 20.575844\n",
      "Number of Epoch: 0052 in 0001 Step cost = 19.263573\n",
      "Number of Epoch: 0052 in 0002 Step cost = 20.422562\n",
      "Number of Epoch: 0052 in 0003 Step cost = 19.744080\n",
      "Number of Epoch: 0052 in 0004 Step cost = 19.764866\n",
      "Number of Epoch: 0052 in 0005 Step cost = 19.560312\n",
      "Number of Epoch: 0052 in 0006 Step cost = 19.294224\n",
      "Number of Epoch: 0052 in 0007 Step cost = 19.946487\n",
      "Number of Epoch: 0052 in 0008 Step cost = 19.741493\n",
      "Number of Epoch: 0053 in 0001 Step cost = 19.982183\n",
      "Number of Epoch: 0053 in 0002 Step cost = 20.031162\n",
      "Number of Epoch: 0053 in 0003 Step cost = 19.513506\n",
      "Number of Epoch: 0053 in 0004 Step cost = 21.925058\n",
      "Number of Epoch: 0053 in 0005 Step cost = 20.321693\n",
      "Number of Epoch: 0053 in 0006 Step cost = 19.850029\n",
      "Number of Epoch: 0053 in 0007 Step cost = 19.949974\n",
      "Number of Epoch: 0053 in 0008 Step cost = 20.512705\n",
      "Number of Epoch: 0054 in 0001 Step cost = 19.963684\n",
      "Number of Epoch: 0054 in 0002 Step cost = 19.024952\n",
      "Number of Epoch: 0054 in 0003 Step cost = 18.889845\n",
      "Number of Epoch: 0054 in 0004 Step cost = 19.150881\n",
      "Number of Epoch: 0054 in 0005 Step cost = 20.233913\n",
      "Number of Epoch: 0054 in 0006 Step cost = 18.608849\n",
      "Number of Epoch: 0054 in 0007 Step cost = 20.720325\n",
      "Number of Epoch: 0054 in 0008 Step cost = 20.055552\n",
      "Number of Epoch: 0055 in 0001 Step cost = 19.056786\n",
      "Number of Epoch: 0055 in 0002 Step cost = 19.091415\n",
      "Number of Epoch: 0055 in 0003 Step cost = 20.779909\n",
      "Number of Epoch: 0055 in 0004 Step cost = 19.996059\n",
      "Number of Epoch: 0055 in 0005 Step cost = 19.622978\n",
      "Number of Epoch: 0055 in 0006 Step cost = 19.764744\n",
      "Number of Epoch: 0055 in 0007 Step cost = 19.663219\n",
      "Number of Epoch: 0055 in 0008 Step cost = 19.260256\n",
      "Number of Epoch: 0056 in 0001 Step cost = 19.214096\n",
      "Number of Epoch: 0056 in 0002 Step cost = 20.129332\n",
      "Number of Epoch: 0056 in 0003 Step cost = 19.862808\n",
      "Number of Epoch: 0056 in 0004 Step cost = 18.575514\n",
      "Number of Epoch: 0056 in 0005 Step cost = 18.330069\n",
      "Number of Epoch: 0056 in 0006 Step cost = 20.281492\n",
      "Number of Epoch: 0056 in 0007 Step cost = 19.307178\n",
      "Number of Epoch: 0056 in 0008 Step cost = 19.109652\n",
      "Number of Epoch: 0057 in 0001 Step cost = 19.237675\n",
      "Number of Epoch: 0057 in 0002 Step cost = 19.687536\n",
      "Number of Epoch: 0057 in 0003 Step cost = 19.379700\n",
      "Number of Epoch: 0057 in 0004 Step cost = 20.418589\n",
      "Number of Epoch: 0057 in 0005 Step cost = 18.623020\n",
      "Number of Epoch: 0057 in 0006 Step cost = 19.094957\n",
      "Number of Epoch: 0057 in 0007 Step cost = 20.164766\n",
      "Number of Epoch: 0057 in 0008 Step cost = 19.304459\n",
      "Number of Epoch: 0058 in 0001 Step cost = 19.514437\n",
      "Number of Epoch: 0058 in 0002 Step cost = 18.669773\n",
      "Number of Epoch: 0058 in 0003 Step cost = 18.342567\n",
      "Number of Epoch: 0058 in 0004 Step cost = 20.087389\n",
      "Number of Epoch: 0058 in 0005 Step cost = 18.743299\n",
      "Number of Epoch: 0058 in 0006 Step cost = 19.494436\n",
      "Number of Epoch: 0058 in 0007 Step cost = 18.664320\n",
      "Number of Epoch: 0058 in 0008 Step cost = 19.675722\n",
      "Number of Epoch: 0059 in 0001 Step cost = 19.446386\n",
      "Number of Epoch: 0059 in 0002 Step cost = 20.356289\n",
      "Number of Epoch: 0059 in 0003 Step cost = 20.749241\n",
      "Number of Epoch: 0059 in 0004 Step cost = 20.482471\n",
      "Number of Epoch: 0059 in 0005 Step cost = 18.467064\n",
      "Number of Epoch: 0059 in 0006 Step cost = 18.465380\n",
      "Number of Epoch: 0059 in 0007 Step cost = 19.119871\n",
      "Number of Epoch: 0059 in 0008 Step cost = 19.261169\n",
      "Number of Epoch: 0060 in 0001 Step cost = 19.317507\n",
      "Number of Epoch: 0060 in 0002 Step cost = 19.430616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch: 0060 in 0003 Step cost = 19.105175\n",
      "Number of Epoch: 0060 in 0004 Step cost = 18.249224\n",
      "Number of Epoch: 0060 in 0005 Step cost = 19.691839\n",
      "Number of Epoch: 0060 in 0006 Step cost = 19.886705\n",
      "Number of Epoch: 0060 in 0007 Step cost = 20.763964\n",
      "Number of Epoch: 0060 in 0008 Step cost = 20.322182\n",
      "Number of Epoch: 0061 in 0001 Step cost = 18.986074\n",
      "Number of Epoch: 0061 in 0002 Step cost = 18.578562\n",
      "Number of Epoch: 0061 in 0003 Step cost = 19.357510\n",
      "Number of Epoch: 0061 in 0004 Step cost = 18.341490\n",
      "Number of Epoch: 0061 in 0005 Step cost = 20.230032\n",
      "Number of Epoch: 0061 in 0006 Step cost = 19.751406\n",
      "Number of Epoch: 0061 in 0007 Step cost = 18.520523\n",
      "Number of Epoch: 0061 in 0008 Step cost = 19.048080\n",
      "Number of Epoch: 0062 in 0001 Step cost = 19.174721\n",
      "Number of Epoch: 0062 in 0002 Step cost = 20.074633\n",
      "Number of Epoch: 0062 in 0003 Step cost = 19.179115\n",
      "Number of Epoch: 0062 in 0004 Step cost = 18.933096\n",
      "Number of Epoch: 0062 in 0005 Step cost = 19.203472\n",
      "Number of Epoch: 0062 in 0006 Step cost = 18.787668\n",
      "Number of Epoch: 0062 in 0007 Step cost = 19.949099\n",
      "Number of Epoch: 0062 in 0008 Step cost = 19.023506\n",
      "Number of Epoch: 0063 in 0001 Step cost = 19.540665\n",
      "Number of Epoch: 0063 in 0002 Step cost = 18.619020\n",
      "Number of Epoch: 0063 in 0003 Step cost = 18.487537\n",
      "Number of Epoch: 0063 in 0004 Step cost = 19.021469\n",
      "Number of Epoch: 0063 in 0005 Step cost = 20.914408\n",
      "Number of Epoch: 0063 in 0006 Step cost = 20.204784\n",
      "Number of Epoch: 0063 in 0007 Step cost = 19.633389\n",
      "Number of Epoch: 0063 in 0008 Step cost = 18.492783\n",
      "Number of Epoch: 0064 in 0001 Step cost = 19.099167\n",
      "Number of Epoch: 0064 in 0002 Step cost = 18.302805\n",
      "Number of Epoch: 0064 in 0003 Step cost = 18.888014\n",
      "Number of Epoch: 0064 in 0004 Step cost = 18.482201\n",
      "Number of Epoch: 0064 in 0005 Step cost = 18.704052\n",
      "Number of Epoch: 0064 in 0006 Step cost = 19.696360\n",
      "Number of Epoch: 0064 in 0007 Step cost = 19.631029\n",
      "Number of Epoch: 0064 in 0008 Step cost = 18.389854\n",
      "Number of Epoch: 0065 in 0001 Step cost = 18.897205\n",
      "Number of Epoch: 0065 in 0002 Step cost = 20.103382\n",
      "Number of Epoch: 0065 in 0003 Step cost = 20.322109\n",
      "Number of Epoch: 0065 in 0004 Step cost = 18.440752\n",
      "Number of Epoch: 0065 in 0005 Step cost = 19.493284\n",
      "Number of Epoch: 0065 in 0006 Step cost = 18.138319\n",
      "Number of Epoch: 0065 in 0007 Step cost = 17.694603\n",
      "Number of Epoch: 0065 in 0008 Step cost = 18.226101\n",
      "Number of Epoch: 0066 in 0001 Step cost = 20.000515\n",
      "Number of Epoch: 0066 in 0002 Step cost = 19.991600\n",
      "Number of Epoch: 0066 in 0003 Step cost = 19.813404\n",
      "Number of Epoch: 0066 in 0004 Step cost = 18.399981\n",
      "Number of Epoch: 0066 in 0005 Step cost = 18.682318\n",
      "Number of Epoch: 0066 in 0006 Step cost = 18.546757\n",
      "Number of Epoch: 0066 in 0007 Step cost = 19.217405\n",
      "Number of Epoch: 0066 in 0008 Step cost = 19.474644\n",
      "Number of Epoch: 0067 in 0001 Step cost = 17.975563\n",
      "Number of Epoch: 0067 in 0002 Step cost = 19.438097\n",
      "Number of Epoch: 0067 in 0003 Step cost = 19.204567\n",
      "Number of Epoch: 0067 in 0004 Step cost = 17.686159\n",
      "Number of Epoch: 0067 in 0005 Step cost = 18.750893\n",
      "Number of Epoch: 0067 in 0006 Step cost = 18.304441\n",
      "Number of Epoch: 0067 in 0007 Step cost = 18.183655\n",
      "Number of Epoch: 0067 in 0008 Step cost = 18.954531\n",
      "Number of Epoch: 0068 in 0001 Step cost = 19.296556\n",
      "Number of Epoch: 0068 in 0002 Step cost = 19.718744\n",
      "Number of Epoch: 0068 in 0003 Step cost = 19.749069\n",
      "Number of Epoch: 0068 in 0004 Step cost = 18.879942\n",
      "Number of Epoch: 0068 in 0005 Step cost = 19.187120\n",
      "Number of Epoch: 0068 in 0006 Step cost = 18.975466\n",
      "Number of Epoch: 0068 in 0007 Step cost = 19.016712\n",
      "Number of Epoch: 0068 in 0008 Step cost = 18.067858\n",
      "Number of Epoch: 0069 in 0001 Step cost = 19.171970\n",
      "Number of Epoch: 0069 in 0002 Step cost = 17.224882\n",
      "Number of Epoch: 0069 in 0003 Step cost = 19.456244\n",
      "Number of Epoch: 0069 in 0004 Step cost = 19.897734\n",
      "Number of Epoch: 0069 in 0005 Step cost = 17.973946\n",
      "Number of Epoch: 0069 in 0006 Step cost = 18.544121\n",
      "Number of Epoch: 0069 in 0007 Step cost = 18.202126\n",
      "Number of Epoch: 0069 in 0008 Step cost = 18.627537\n",
      "Number of Epoch: 0070 in 0001 Step cost = 17.611319\n",
      "Number of Epoch: 0070 in 0002 Step cost = 17.118896\n",
      "Number of Epoch: 0070 in 0003 Step cost = 19.791679\n",
      "Number of Epoch: 0070 in 0004 Step cost = 17.680552\n",
      "Number of Epoch: 0070 in 0005 Step cost = 20.217836\n",
      "Number of Epoch: 0070 in 0006 Step cost = 17.950171\n",
      "Number of Epoch: 0070 in 0007 Step cost = 18.372194\n",
      "Number of Epoch: 0070 in 0008 Step cost = 18.820450\n",
      "Number of Epoch: 0071 in 0001 Step cost = 17.576250\n",
      "Number of Epoch: 0071 in 0002 Step cost = 17.980913\n",
      "Number of Epoch: 0071 in 0003 Step cost = 18.737942\n",
      "Number of Epoch: 0071 in 0004 Step cost = 18.827553\n",
      "Number of Epoch: 0071 in 0005 Step cost = 18.543430\n",
      "Number of Epoch: 0071 in 0006 Step cost = 18.940966\n",
      "Number of Epoch: 0071 in 0007 Step cost = 17.676651\n",
      "Number of Epoch: 0071 in 0008 Step cost = 17.701967\n",
      "Number of Epoch: 0072 in 0001 Step cost = 18.979630\n",
      "Number of Epoch: 0072 in 0002 Step cost = 19.491728\n",
      "Number of Epoch: 0072 in 0003 Step cost = 18.451372\n",
      "Number of Epoch: 0072 in 0004 Step cost = 19.113602\n",
      "Number of Epoch: 0072 in 0005 Step cost = 17.852655\n",
      "Number of Epoch: 0072 in 0006 Step cost = 18.590855\n",
      "Number of Epoch: 0072 in 0007 Step cost = 19.250513\n",
      "Number of Epoch: 0072 in 0008 Step cost = 18.213428\n",
      "Number of Epoch: 0073 in 0001 Step cost = 18.641146\n",
      "Number of Epoch: 0073 in 0002 Step cost = 18.872915\n",
      "Number of Epoch: 0073 in 0003 Step cost = 19.537973\n",
      "Number of Epoch: 0073 in 0004 Step cost = 18.260862\n",
      "Number of Epoch: 0073 in 0005 Step cost = 19.165018\n",
      "Number of Epoch: 0073 in 0006 Step cost = 17.714235\n",
      "Number of Epoch: 0073 in 0007 Step cost = 18.896536\n",
      "Number of Epoch: 0073 in 0008 Step cost = 17.832632\n",
      "Number of Epoch: 0074 in 0001 Step cost = 16.637587\n",
      "Number of Epoch: 0074 in 0002 Step cost = 19.144547\n",
      "Number of Epoch: 0074 in 0003 Step cost = 17.464611\n",
      "Number of Epoch: 0074 in 0004 Step cost = 18.633713\n",
      "Number of Epoch: 0074 in 0005 Step cost = 17.277359\n",
      "Number of Epoch: 0074 in 0006 Step cost = 18.602861\n",
      "Number of Epoch: 0074 in 0007 Step cost = 18.263048\n",
      "Number of Epoch: 0074 in 0008 Step cost = 17.640675\n",
      "Number of Epoch: 0075 in 0001 Step cost = 18.148098\n",
      "Number of Epoch: 0075 in 0002 Step cost = 16.772829\n",
      "Number of Epoch: 0075 in 0003 Step cost = 18.125122\n",
      "Number of Epoch: 0075 in 0004 Step cost = 17.713755\n",
      "Number of Epoch: 0075 in 0005 Step cost = 19.275076\n",
      "Number of Epoch: 0075 in 0006 Step cost = 18.437937\n",
      "Number of Epoch: 0075 in 0007 Step cost = 17.558218\n",
      "Number of Epoch: 0075 in 0008 Step cost = 18.501183\n",
      "Number of Epoch: 0076 in 0001 Step cost = 19.057766\n",
      "Number of Epoch: 0076 in 0002 Step cost = 18.693218\n",
      "Number of Epoch: 0076 in 0003 Step cost = 17.969067\n",
      "Number of Epoch: 0076 in 0004 Step cost = 18.286345\n",
      "Number of Epoch: 0076 in 0005 Step cost = 17.999037\n",
      "Number of Epoch: 0076 in 0006 Step cost = 17.848228\n",
      "Number of Epoch: 0076 in 0007 Step cost = 18.043566\n",
      "Number of Epoch: 0076 in 0008 Step cost = 17.907616\n",
      "Number of Epoch: 0077 in 0001 Step cost = 18.813231\n",
      "Number of Epoch: 0077 in 0002 Step cost = 17.892298\n",
      "Number of Epoch: 0077 in 0003 Step cost = 16.630739\n",
      "Number of Epoch: 0077 in 0004 Step cost = 18.716579\n",
      "Number of Epoch: 0077 in 0005 Step cost = 17.395187\n",
      "Number of Epoch: 0077 in 0006 Step cost = 18.386839\n",
      "Number of Epoch: 0077 in 0007 Step cost = 18.239101\n",
      "Number of Epoch: 0077 in 0008 Step cost = 18.294727\n",
      "Number of Epoch: 0078 in 0001 Step cost = 18.503025\n",
      "Number of Epoch: 0078 in 0002 Step cost = 18.305809\n",
      "Number of Epoch: 0078 in 0003 Step cost = 17.654472\n",
      "Number of Epoch: 0078 in 0004 Step cost = 18.675938\n",
      "Number of Epoch: 0078 in 0005 Step cost = 18.604332\n",
      "Number of Epoch: 0078 in 0006 Step cost = 17.336107\n",
      "Number of Epoch: 0078 in 0007 Step cost = 16.756105\n",
      "Number of Epoch: 0078 in 0008 Step cost = 18.166920\n",
      "Number of Epoch: 0079 in 0001 Step cost = 16.620047\n",
      "Number of Epoch: 0079 in 0002 Step cost = 18.441484\n",
      "Number of Epoch: 0079 in 0003 Step cost = 18.108982\n",
      "Number of Epoch: 0079 in 0004 Step cost = 18.719711\n",
      "Number of Epoch: 0079 in 0005 Step cost = 18.905325\n",
      "Number of Epoch: 0079 in 0006 Step cost = 18.264877\n",
      "Number of Epoch: 0079 in 0007 Step cost = 17.880816\n",
      "Number of Epoch: 0079 in 0008 Step cost = 17.582237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch: 0080 in 0001 Step cost = 17.920820\n",
      "Number of Epoch: 0080 in 0002 Step cost = 17.275873\n",
      "Number of Epoch: 0080 in 0003 Step cost = 17.580505\n",
      "Number of Epoch: 0080 in 0004 Step cost = 17.678110\n",
      "Number of Epoch: 0080 in 0005 Step cost = 17.698721\n",
      "Number of Epoch: 0080 in 0006 Step cost = 17.278587\n",
      "Number of Epoch: 0080 in 0007 Step cost = 18.581358\n",
      "Number of Epoch: 0080 in 0008 Step cost = 19.070055\n",
      "Number of Epoch: 0081 in 0001 Step cost = 17.488075\n",
      "Number of Epoch: 0081 in 0002 Step cost = 17.798361\n",
      "Number of Epoch: 0081 in 0003 Step cost = 18.233534\n",
      "Number of Epoch: 0081 in 0004 Step cost = 17.953636\n",
      "Number of Epoch: 0081 in 0005 Step cost = 18.476866\n",
      "Number of Epoch: 0081 in 0006 Step cost = 18.360834\n",
      "Number of Epoch: 0081 in 0007 Step cost = 18.268564\n",
      "Number of Epoch: 0081 in 0008 Step cost = 17.477198\n",
      "Number of Epoch: 0082 in 0001 Step cost = 18.965200\n",
      "Number of Epoch: 0082 in 0002 Step cost = 17.241982\n",
      "Number of Epoch: 0082 in 0003 Step cost = 17.328848\n",
      "Number of Epoch: 0082 in 0004 Step cost = 17.759789\n",
      "Number of Epoch: 0082 in 0005 Step cost = 18.355492\n",
      "Number of Epoch: 0082 in 0006 Step cost = 18.133734\n",
      "Number of Epoch: 0082 in 0007 Step cost = 17.767767\n",
      "Number of Epoch: 0082 in 0008 Step cost = 18.224060\n",
      "Number of Epoch: 0083 in 0001 Step cost = 16.951843\n",
      "Number of Epoch: 0083 in 0002 Step cost = 17.986399\n",
      "Number of Epoch: 0083 in 0003 Step cost = 17.981535\n",
      "Number of Epoch: 0083 in 0004 Step cost = 18.637672\n",
      "Number of Epoch: 0083 in 0005 Step cost = 17.943970\n",
      "Number of Epoch: 0083 in 0006 Step cost = 16.912752\n",
      "Number of Epoch: 0083 in 0007 Step cost = 16.657339\n",
      "Number of Epoch: 0083 in 0008 Step cost = 18.510689\n",
      "Number of Epoch: 0084 in 0001 Step cost = 17.346682\n",
      "Number of Epoch: 0084 in 0002 Step cost = 17.666506\n",
      "Number of Epoch: 0084 in 0003 Step cost = 17.228844\n",
      "Number of Epoch: 0084 in 0004 Step cost = 17.657225\n",
      "Number of Epoch: 0084 in 0005 Step cost = 18.534252\n",
      "Number of Epoch: 0084 in 0006 Step cost = 18.503323\n",
      "Number of Epoch: 0084 in 0007 Step cost = 17.630617\n",
      "Number of Epoch: 0084 in 0008 Step cost = 17.087759\n",
      "Number of Epoch: 0085 in 0001 Step cost = 17.712626\n",
      "Number of Epoch: 0085 in 0002 Step cost = 17.464073\n",
      "Number of Epoch: 0085 in 0003 Step cost = 17.253933\n",
      "Number of Epoch: 0085 in 0004 Step cost = 16.969254\n",
      "Number of Epoch: 0085 in 0005 Step cost = 18.043032\n",
      "Number of Epoch: 0085 in 0006 Step cost = 18.119684\n",
      "Number of Epoch: 0085 in 0007 Step cost = 17.446054\n",
      "Number of Epoch: 0085 in 0008 Step cost = 18.148878\n",
      "Number of Epoch: 0086 in 0001 Step cost = 18.060255\n",
      "Number of Epoch: 0086 in 0002 Step cost = 17.861280\n",
      "Number of Epoch: 0086 in 0003 Step cost = 18.247826\n",
      "Number of Epoch: 0086 in 0004 Step cost = 17.079100\n",
      "Number of Epoch: 0086 in 0005 Step cost = 17.958523\n",
      "Number of Epoch: 0086 in 0006 Step cost = 16.667391\n",
      "Number of Epoch: 0086 in 0007 Step cost = 17.295504\n",
      "Number of Epoch: 0086 in 0008 Step cost = 17.193153\n",
      "Number of Epoch: 0087 in 0001 Step cost = 18.697275\n",
      "Number of Epoch: 0087 in 0002 Step cost = 18.064941\n",
      "Number of Epoch: 0087 in 0003 Step cost = 17.888966\n",
      "Number of Epoch: 0087 in 0004 Step cost = 17.549599\n",
      "Number of Epoch: 0087 in 0005 Step cost = 17.961958\n",
      "Number of Epoch: 0087 in 0006 Step cost = 17.278969\n",
      "Number of Epoch: 0087 in 0007 Step cost = 17.810642\n",
      "Number of Epoch: 0087 in 0008 Step cost = 18.594351\n",
      "Number of Epoch: 0088 in 0001 Step cost = 18.009146\n",
      "Number of Epoch: 0088 in 0002 Step cost = 17.806120\n",
      "Number of Epoch: 0088 in 0003 Step cost = 17.559219\n",
      "Number of Epoch: 0088 in 0004 Step cost = 19.073442\n",
      "Number of Epoch: 0088 in 0005 Step cost = 17.867750\n",
      "Number of Epoch: 0088 in 0006 Step cost = 16.565355\n",
      "Number of Epoch: 0088 in 0007 Step cost = 17.371469\n",
      "Number of Epoch: 0088 in 0008 Step cost = 17.984570\n",
      "Number of Epoch: 0089 in 0001 Step cost = 16.794786\n",
      "Number of Epoch: 0089 in 0002 Step cost = 18.979477\n",
      "Number of Epoch: 0089 in 0003 Step cost = 16.646229\n",
      "Number of Epoch: 0089 in 0004 Step cost = 17.393383\n",
      "Number of Epoch: 0089 in 0005 Step cost = 17.871859\n",
      "Number of Epoch: 0089 in 0006 Step cost = 17.292021\n",
      "Number of Epoch: 0089 in 0007 Step cost = 18.373718\n",
      "Number of Epoch: 0089 in 0008 Step cost = 17.392221\n",
      "Number of Epoch: 0090 in 0001 Step cost = 18.572924\n",
      "Number of Epoch: 0090 in 0002 Step cost = 17.396273\n",
      "Number of Epoch: 0090 in 0003 Step cost = 16.563421\n",
      "Number of Epoch: 0090 in 0004 Step cost = 17.121700\n",
      "Number of Epoch: 0090 in 0005 Step cost = 18.694004\n",
      "Number of Epoch: 0090 in 0006 Step cost = 18.685207\n",
      "Number of Epoch: 0090 in 0007 Step cost = 17.851982\n",
      "Number of Epoch: 0090 in 0008 Step cost = 17.605585\n",
      "Number of Epoch: 0091 in 0001 Step cost = 16.537586\n",
      "Number of Epoch: 0091 in 0002 Step cost = 18.533409\n",
      "Number of Epoch: 0091 in 0003 Step cost = 16.458427\n",
      "Number of Epoch: 0091 in 0004 Step cost = 17.125841\n",
      "Number of Epoch: 0091 in 0005 Step cost = 17.113962\n",
      "Number of Epoch: 0091 in 0006 Step cost = 17.707901\n",
      "Number of Epoch: 0091 in 0007 Step cost = 17.155910\n",
      "Number of Epoch: 0091 in 0008 Step cost = 18.576111\n",
      "Number of Epoch: 0092 in 0001 Step cost = 17.757689\n",
      "Number of Epoch: 0092 in 0002 Step cost = 17.584177\n",
      "Number of Epoch: 0092 in 0003 Step cost = 17.540989\n",
      "Number of Epoch: 0092 in 0004 Step cost = 18.302261\n",
      "Number of Epoch: 0092 in 0005 Step cost = 18.081772\n",
      "Number of Epoch: 0092 in 0006 Step cost = 16.415937\n",
      "Number of Epoch: 0092 in 0007 Step cost = 17.099548\n",
      "Number of Epoch: 0092 in 0008 Step cost = 17.217285\n",
      "Number of Epoch: 0093 in 0001 Step cost = 16.582237\n",
      "Number of Epoch: 0093 in 0002 Step cost = 18.329800\n",
      "Number of Epoch: 0093 in 0003 Step cost = 17.536734\n",
      "Number of Epoch: 0093 in 0004 Step cost = 17.893482\n",
      "Number of Epoch: 0093 in 0005 Step cost = 16.652586\n",
      "Number of Epoch: 0093 in 0006 Step cost = 17.325901\n",
      "Number of Epoch: 0093 in 0007 Step cost = 19.472702\n",
      "Number of Epoch: 0093 in 0008 Step cost = 17.945541\n",
      "Number of Epoch: 0094 in 0001 Step cost = 16.232336\n",
      "Number of Epoch: 0094 in 0002 Step cost = 17.371811\n",
      "Number of Epoch: 0094 in 0003 Step cost = 17.482079\n",
      "Number of Epoch: 0094 in 0004 Step cost = 18.322414\n",
      "Number of Epoch: 0094 in 0005 Step cost = 17.376320\n",
      "Number of Epoch: 0094 in 0006 Step cost = 18.609529\n",
      "Number of Epoch: 0094 in 0007 Step cost = 16.785751\n",
      "Number of Epoch: 0094 in 0008 Step cost = 17.557035\n",
      "Number of Epoch: 0095 in 0001 Step cost = 17.494131\n",
      "Number of Epoch: 0095 in 0002 Step cost = 16.281847\n",
      "Number of Epoch: 0095 in 0003 Step cost = 15.866761\n",
      "Number of Epoch: 0095 in 0004 Step cost = 16.297976\n",
      "Number of Epoch: 0095 in 0005 Step cost = 17.434410\n",
      "Number of Epoch: 0095 in 0006 Step cost = 17.827150\n",
      "Number of Epoch: 0095 in 0007 Step cost = 17.114502\n",
      "Number of Epoch: 0095 in 0008 Step cost = 16.955017\n",
      "Number of Epoch: 0096 in 0001 Step cost = 17.534712\n",
      "Number of Epoch: 0096 in 0002 Step cost = 16.823219\n",
      "Number of Epoch: 0096 in 0003 Step cost = 16.498716\n",
      "Number of Epoch: 0096 in 0004 Step cost = 15.908147\n",
      "Number of Epoch: 0096 in 0005 Step cost = 17.776186\n",
      "Number of Epoch: 0096 in 0006 Step cost = 16.634512\n",
      "Number of Epoch: 0096 in 0007 Step cost = 17.751543\n",
      "Number of Epoch: 0096 in 0008 Step cost = 17.611057\n",
      "Number of Epoch: 0097 in 0001 Step cost = 17.796432\n",
      "Number of Epoch: 0097 in 0002 Step cost = 17.833311\n",
      "Number of Epoch: 0097 in 0003 Step cost = 17.403175\n",
      "Number of Epoch: 0097 in 0004 Step cost = 17.556885\n",
      "Number of Epoch: 0097 in 0005 Step cost = 17.650103\n",
      "Number of Epoch: 0097 in 0006 Step cost = 16.756182\n",
      "Number of Epoch: 0097 in 0007 Step cost = 16.542328\n",
      "Number of Epoch: 0097 in 0008 Step cost = 17.690130\n",
      "Number of Epoch: 0098 in 0001 Step cost = 16.957836\n",
      "Number of Epoch: 0098 in 0002 Step cost = 18.467918\n",
      "Number of Epoch: 0098 in 0003 Step cost = 17.104980\n",
      "Number of Epoch: 0098 in 0004 Step cost = 18.610231\n",
      "Number of Epoch: 0098 in 0005 Step cost = 16.517845\n",
      "Number of Epoch: 0098 in 0006 Step cost = 17.453810\n",
      "Number of Epoch: 0098 in 0007 Step cost = 16.956556\n",
      "Number of Epoch: 0098 in 0008 Step cost = 17.008642\n",
      "Number of Epoch: 0099 in 0001 Step cost = 16.149643\n",
      "Number of Epoch: 0099 in 0002 Step cost = 16.905886\n",
      "Number of Epoch: 0099 in 0003 Step cost = 16.356400\n",
      "Number of Epoch: 0099 in 0004 Step cost = 15.181056\n",
      "Number of Epoch: 0099 in 0005 Step cost = 16.089928\n",
      "Number of Epoch: 0099 in 0006 Step cost = 17.166613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch: 0099 in 0007 Step cost = 15.909819\n",
      "Number of Epoch: 0099 in 0008 Step cost = 16.777527\n",
      "Number of Epoch: 0100 in 0001 Step cost = 17.625647\n",
      "Number of Epoch: 0100 in 0002 Step cost = 17.465992\n",
      "Number of Epoch: 0100 in 0003 Step cost = 16.320839\n",
      "Number of Epoch: 0100 in 0004 Step cost = 17.411287\n",
      "Number of Epoch: 0100 in 0005 Step cost = 16.500607\n",
      "Number of Epoch: 0100 in 0006 Step cost = 16.382555\n",
      "Number of Epoch: 0100 in 0007 Step cost = 16.728926\n",
      "Number of Epoch: 0100 in 0008 Step cost = 18.038315\n"
     ]
    }
   ],
   "source": [
    "for num_epoch in range(num_epoch):\n",
    "    mems = None\n",
    "    # \n",
    "    features = data_utils._create_data(sp=sp,\n",
    "                                       input_paths=data_path,\n",
    "                                       seq_len=seq_len,\n",
    "                                       reuse_len=reuse_len,\n",
    "                                       bi_data=bi_data,\n",
    "                                       num_predict=num_predict,\n",
    "                                       mask_alpha=mask_alpha,\n",
    "                                       mask_beta=mask_beta)\n",
    "\n",
    "    num_step = 0\n",
    "    # Permutation Language Modeling Objective로 인한 학습 과정\n",
    "    for feature in features:\n",
    "        permutation = data_utils.make_permute(feature,\n",
    "                                              reuse_len=reuse_len,\n",
    "                                              seq_len=seq_len,\n",
    "                                              perm_size=perm_size,\n",
    "                                              num_predict=num_predict)\n",
    "        \n",
    "        # batch size is 1 \n",
    "        # Target-Aware Representation for Transformer를 위해 argument 설정\n",
    "        # Two-Stream Self-Attention를 위해 argument 설정\n",
    "        inp_k = permutation['input_k'].unsqueeze(-1) # [seq_len, 1(=bsz)]\n",
    "        seg_id = permutation['seg_id'].unsqueeze(-1) # [seq_len, 1(=bsz)]\n",
    "        target = permutation['target'].unsqueeze(-1) # [num_predict, 1(=bsz)]\n",
    "        perm_mask = permutation['perm_mask'].unsqueeze(-1) # [seq_len, seq_len, 1(=bsz)]\n",
    "        target_mapping = permutation['target_mapping'].unsqueeze(-1) # [num_predict, seq_len, 1(=bsz)]\n",
    "        inp_q = permutation['input_q'].unsqueeze(-1) # [seq_len, 1(=bsz)]\n",
    "        tgt_mask = permutation['target_mask'].unsqueeze(-1) # [num_predict, 1(=bsz)]\n",
    "        \n",
    "        \n",
    "        logits, new_mems = model(inp_k=inp_k, seg_id=seg_id, input_mask=None,\n",
    "                                 mems=mems, perm_mask=perm_mask,\n",
    "                                 target_mapping=target_mapping, inp_q=inp_q)\n",
    "        \n",
    "        lm_loss = criterion(logits.transpose(1, 2), target).type(torch.float32)\n",
    "        tgt_mask_sum = tgt_mask.reshape(-1).sum()\n",
    "        lm_loss_sum = (lm_loss * tgt_mask).reshape(-1).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss = lm_loss_sum / tgt_mask_sum\n",
    "        print('Number of Epoch: %04d in %04d Step' % ((num_epoch + 1), (num_step + 1)),\n",
    "              'cost =', '{:.6f}'.format(total_loss))\n",
    "        num_step += 1\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 현재 세그먼트를 메모리에 저장\n",
    "        mems = new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a917db",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- [xlnet-Pytorch](https://github.com/graykode/xlnet-Pytorch) : 해당 코드 참고\n",
    "- [XLNet Fine-Tuning with PyTorch](https://www.kaggle.com/code/jaskaransingh/xlnet-fine-tuning-with-pytorch) : 새로운 데이터셋으로 학습하는 경우 참고\n",
    "- [[Paper Review] Transformer to T5 (XLNet, RoBERTa, MASS, BART, MT-DNN,T5)](https://www.youtube.com/watch?v=v7diENO2mEA) : 이론 참고"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.364px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
