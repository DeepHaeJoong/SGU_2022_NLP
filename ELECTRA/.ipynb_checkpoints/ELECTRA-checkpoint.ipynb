{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2376bd",
   "metadata": {},
   "source": [
    "### ELECTRA : Efficiently Learning an Encodeer that classifies Token Replacement Accurately\n",
    "\n",
    "- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR’20](https://arxiv.org/abs/2003.10555)\n",
    "- 작성자 : 220200013 이해중\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c082b9",
   "metadata": {},
   "source": [
    "기존의 BERT, GPT-2 모델과 달리 **다른 형태(GAN : generative adversial network과 유사)** 로 pre-trained 된 모델로 ELECTRA 학습을 위한 전체 모델 구조는 Genrator와 Discriminator로, 두 개의 네트워크를 필요로 한다.\n",
    "\n",
    "- Generator : `BERT`에서 사용된 마스크 언어 모델(**M**asked **L**anguage **M**odel)과 동일하게 구성됨 (언어의 특성을 잘 학습할 수 있도록 함)\n",
    "- Discriminator : Discriminator는 입력 토큰 시퀀스에 대해서 각 토큰이 **original**인지 **replaced**인지 binary classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d1cce",
   "metadata": {},
   "source": [
    "#### Generator \n",
    "\n",
    "1. 입력 $\\bf{x}$ = $[x_1, x_2, ... , x_n]$에 대해서 마스킹할 위치의 집합 $\\bf{m}$ = $[m_1, m_2, m_3, ... , m_k]$을 결정한다.\n",
    "    - 모든 마스킹 위치는 $1$과 $n$ 사이의 정수로, 다음과 같이 uniform distribution을 사용하여 할당할 수 있다.\n",
    "    - $m_i \\sim uniform (1, n) \\text{ for } i = 1 \\text{ to } k $\n",
    "    - 마스킹할 $k$ 수는 보통 $\\frac{15}{100}\\times n$을 사용한다. (전체 토큰의 15%)\n",
    "    \n",
    "    \n",
    "    \n",
    "2. 결정한 위치에 있는 입력 토큰을 $[MASK]$로 치환한다.\n",
    "    - 이 과정을 $\\bf{x}^{masked} = REPLACE$ $(\\bf{x}, \\bf{m},$ $[MASK] )$로 표현\n",
    "3. 마스킹 된 입력 $\\bf{x}^{masked}$에 대해서 **generator**는 아래와 같이 원래의 토큰이 무엇인지 예측할 수 있다.\n",
    "    - 이 과정을 다음과 같이 표현할 수 있다. ($t$ 번째 토큰에 대한 예측)\n",
    "    \n",
    "$$\n",
    "p_G\\left(x_t \\mid \\bf{x}^{masked}\\right)=\\exp \\left(e\\left(x_t\\right)^T h_G(\\bf{x}^{masked})_t\\right) / \\sum_{x^{\\prime}} \\exp \\left(e\\left(x^{\\prime}\\right)^T h_G(\\bf{x}^{masked})_t\\right)\n",
    "$$\n",
    "\n",
    " 위 식에서 $e(\\cdot)$는 임베딩을 의미하며, 위 식은 language model의 출력 레이어와 임베딩 레이어의 가중치를 공유(weight sharing)하겠다는 의미\n",
    "\n",
    "4. 최종적으로 아래와 같은 MLM loss로 학습한다.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)=\\mathbb{E}\\left(\\sum_{i \\in \\mathbf{m}}-\\log p_G\\left(x_i \\mid \\mathbf{x}^{\\text {masked }}\\right)\\right)\n",
    "$$\n",
    "\n",
    "![image.png](https://github.com/DeepHaeJoong/SGU_2022_NLP/blob/master/image/electra_01.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aeee67",
   "metadata": {},
   "source": [
    "####  Discriminator\n",
    "\n",
    "Discriminator는 입력 토큰 시퀀스에 대해서 각 토큰이 **original**인지 **replaced**인지 binary classification으로 학습하며, 구체적인 학습 매커니즘은 다음과 같다.\n",
    "\n",
    "1. Generator를 이용해서 마스킹 된 입력 토큰을 예측하게 된다. (Generator의 학습 메커니즘 1~3 단계 해당됨)\n",
    "2. Generator에서 마스킹할 위치의 집합 $\\bf{m}$에 해당되는 위치의 토큰을 $[MASK]$가 아닌 generator의 softmax 분포 $p_{G}(x_t|\\bf{x})$d에 대해 샘플링한 토큰으로 치환(corrupt)함\n",
    "    - Original input : [**'the'** , 'chef', **'cooked'** , 'the', 'meal']\n",
    "    - Masked input (Input for generator) : [**[MASK]**, 'chef', **[MASK]**, 'the', 'meal']\n",
    "    - Input (Input for discriminator) : [$\\color{blue}{\\text{'the'}}$, 'chef', $\\color{red}{\\text{'ate'}}$, 'the', 'meal']\n",
    "    \n",
    "        - 첫 번째 단어 'the'는 샘플링했는데 원래 입력 토큰 'the'와 동일하게 출력된 경우\n",
    "        - 세 번째 단어 'ate'는 샘플링했는데 원래 입력 토큰 'cooked'와 다르게 출력된 경우\n",
    "        \n",
    "    - 이러한 치환 과정은 다음과 같이 정리할 수 있다.\n",
    "    \n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\mathbf{x}^{\\text {corrupt }}=\\mathrm{REPLACE}(\\mathbf{x}, \\mathbf{m}, \\hat{\\mathbf{x}}) \\\\\n",
    "\\hat{\\mathbf{x}} \\sim p_G\\left(x_i \\mid \\mathbf{x}^{\\text {masked }}\\right) \\text { for } i \\in \\mathbf{m}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "3. 치환된 입력 $\\bf{x}^{corrupt}$에 대해서 discriminator는 아래와 같이 각 토큰이 원래 입력과 동일한지 치환된 것인지 예측하게 된다.\n",
    "\n",
    "    - Target class \n",
    "        - Original : 이 위치에 해당하는 토큰은 원본 문장의 토큰과 같은 것\n",
    "        - Replaced : 이 위치에 해당하는 토큰은 generator에 의해 변형된 것\n",
    "        \n",
    "    - 이런 과정을 구학적으로 표현하면 다음과 같다. ($t$번째 토큰에 대한 예측)\n",
    "$$\n",
    "D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)=\\operatorname{sigmoid}\\left(w^T h_D\\left(\\mathbf{x}^{\\text {corrupt }}\\right)_t\\right)\n",
    "$$\n",
    "\n",
    "4. 최종적으로 Discriminator의 Loss는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathcal{L}_{D i s c}\\left(\\mathbf{x}, \\theta_D\\right) \\\\\n",
    "& =\\mathbb{E}\\left(\\sum_{t=1}^n-1\\left(x_t^{\\text {corrupt }}=x_t\\right) \\log D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)-1\\left(x_t^{\\text {corrupt }} \\neq x_t\\right) \\log \\left(1-D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)\\right)\\right. \\\\\n",
    "& ) \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767feb9",
   "metadata": {},
   "source": [
    "최종적으로 ELECTRA는 대용량 corpus $\\mathcal{X}$에 대해서 generator loss와 discriminator loss의 합을 최소화하는 방향으로 학습하게 된다. $\\lambda = 50$으로 설정했다고 한다.\n",
    "\n",
    "$$\n",
    "\\min _{\\theta_G, \\theta_D} \\sum_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)+\\lambda \\mathcal{L}_{\\text {Disc }}\\left(\\mathbf{x}, \\theta_D\\right)\n",
    "$$\n",
    "\n",
    "Generator 에서 샘플링 과정이 있기 때문에 discriminator loss는 generator로 역전파 되지 않으며, 위의 구조로 pre-training을 마친 뒤에 generator는 버리고 discriminator만 취해서 downstream task으로 fine-tuning을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a139511",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5bde4",
   "metadata": {},
   "source": [
    "The following example uses reformer-pytorch, which is available to be pip installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03c092e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:25:27.923948Z",
     "start_time": "2022-12-30T04:25:27.911941Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install electra-pytorch\n",
    "#!pip install reformer_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87856886",
   "metadata": {},
   "source": [
    "#### Electra class 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90767ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:27:47.007179Z",
     "start_time": "2022-12-30T04:27:46.996648Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "# Electra class는 loss, MLM loss, ..등의 다양한 값들을 tuple type으로 return 함  \n",
    "Results = namedtuple('Results', [\n",
    "    'loss',\n",
    "    'mlm_loss',\n",
    "    'disc_loss',\n",
    "    'gen_acc',\n",
    "    'disc_acc',\n",
    "    'disc_labels',\n",
    "    'disc_predictions'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1.):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a93161f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:51:29.740721Z",
     "start_time": "2022-12-30T04:51:29.725692Z"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "\n",
    "# main electra class\n",
    "\n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "\n",
    "        # get generator output and get mlm loss\n",
    "        logits = self.generator(masked_input, **kwargs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator(disc_input, **kwargs)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d38f9b",
   "metadata": {},
   "source": [
    "#### generator 및 discriminator instance 정의  \n",
    "\n",
    "- `reformer_pytorch` library에서 제공되는 \"Efficient Transformer\" 모델을 사용함\n",
    "\n",
    "\n",
    "(1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator : (일반적으로 generator의 크기는 discriminator 크기의 1/4배로 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a064ebf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:49:23.448277Z",
     "start_time": "2022-12-30T04:49:23.442783Z"
    }
   },
   "outputs": [],
   "source": [
    "from reformer_pytorch import ReformerLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f83f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:48:55.576761Z",
     "start_time": "2022-12-30T04:48:55.501263Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = ReformerLM(\n",
    "    num_tokens = 20000,\n",
    "    emb_dim = 128,\n",
    "    dim = 256,              # smaller hidden dimension\n",
    "    heads = 4,              # less heads\n",
    "    ff_mult = 2,            # smaller feed forward intermediate dimension\n",
    "    dim_head = 64,\n",
    "    depth = 12,\n",
    "    max_seq_len = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f7281c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:48:57.134028Z",
     "start_time": "2022-12-30T04:48:56.523992Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = ReformerLM(\n",
    "    num_tokens = 20000,\n",
    "    emb_dim = 128,\n",
    "    dim = 1024,\n",
    "    dim_head = 64,\n",
    "    heads = 16,\n",
    "    depth = 12,\n",
    "    ff_mult = 4,\n",
    "    max_seq_len = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71644d",
   "metadata": {},
   "source": [
    "#### generator와 discriminator의 embedding 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17de750",
   "metadata": {},
   "source": [
    "(2) weight tie the token and positional embeddings of generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05083aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:48:58.272533Z",
     "start_time": "2022-12-30T04:48:58.267534Z"
    }
   },
   "outputs": [],
   "source": [
    "generator.token_emb = discriminator.token_emb\n",
    "generator.pos_emb = discriminator.pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e56a5",
   "metadata": {},
   "source": [
    "#### instantiate electra \n",
    "\n",
    "(3) instantiate electra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de840284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:51:33.510573Z",
     "start_time": "2022-12-30T04:51:33.492840Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Electra(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    discr_dim = 1024,           # the embedding dimension of the discriminator\n",
    "    discr_layer = 'reformer',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    mask_token_id = 2,          # the token id reserved for masking\n",
    "    pad_token_id = 0,           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    mask_ignore_token_ids = []  # ids of tokens to ignore for mask modeling ex. (cls, sep)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc3036",
   "metadata": {},
   "source": [
    "#### train 과정 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f70c337c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:53:16.865098Z",
     "start_time": "2022-12-30T04:52:49.225342Z"
    }
   },
   "outputs": [],
   "source": [
    "data = torch.randint(0, 20000, (1, 1024))\n",
    "\n",
    "results = trainer(data)\n",
    "results.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1316c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-30T04:53:23.619470Z",
     "start_time": "2022-12-30T04:53:23.599797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results(loss=tensor(47.5575, grad_fn=<AddBackward0>), mlm_loss=tensor(9.9434, grad_fn=<NllLoss2DBackward0>), disc_loss=tensor(0.7523, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), gen_acc=tensor(0.), disc_acc=tensor(0.5189), disc_labels=tensor([[0., 0., 0.,  ..., 0., 0., 0.]]), disc_predictions=tensor([[1., 0., 0.,  ..., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea9fa2",
   "metadata": {},
   "source": [
    "### Reference \n",
    "\n",
    "- https://github.com/lucidrains/electra-pytorch\n",
    "- https://github.com/lucidrains/reformer-pytorch"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "388.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
